{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bc4792-b763-4750-b6fc-7cc9f0fc8d01",
   "metadata": {},
   "source": [
    "Basic Graph Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a75c44-3c42-46e4-b4ab-0316ab038a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict, Any, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Neo4jGraphRetriever:\n",
    "    def __init__(self, neo4j_graph):\n",
    "        \"\"\"\n",
    "        Initialize the Neo4j graph retriever\n",
    "\n",
    "        Args:\n",
    "            neo4j_graph: Your Neo4j graph object\n",
    "        \"\"\"\n",
    "        self.graph = neo4j_graph\n",
    "        # Create a lowercase mapping for better entity matching\n",
    "        self.node_mapping = self._create_node_mapping()\n",
    "\n",
    "    def _create_node_mapping(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Create a mapping from lowercase text to actual node names for better matching\"\"\"\n",
    "        mapping = defaultdict(list)\n",
    "\n",
    "        # Query to get all nodes and their properties\n",
    "        query = \"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN n.name as name, labels(n) as labels, id(n) as id\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.graph.query(query)\n",
    "            for record in result:\n",
    "                node_name = record.get('name', '')\n",
    "                node_labels = record.get('labels', [])\n",
    "                node_id = record.get('id', '')\n",
    "\n",
    "                # Create searchable text from name and labels\n",
    "                searchable_texts = []\n",
    "                if node_name:\n",
    "                    searchable_texts.append(str(node_name))\n",
    "                searchable_texts.extend([str(label) for label in node_labels])\n",
    "\n",
    "                for text in searchable_texts:\n",
    "                    if text:\n",
    "                        text_lower = text.lower()\n",
    "                        mapping[text_lower].append(node_name or f\"node_{node_id}\")\n",
    "\n",
    "                        # Also add partial matches (split by spaces, hyphens, etc.)\n",
    "                        words = text_lower.replace('-', ' ').replace('_', ' ').split()\n",
    "                        for word in words:\n",
    "                            if len(word) > 2:  # Avoid very short words\n",
    "                                mapping[word].append(node_name or f\"node_{node_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating node mapping: {e}\")\n",
    "            # Fallback: try a simpler query\n",
    "            try:\n",
    "                simple_query = \"MATCH (n) RETURN n LIMIT 10\"\n",
    "                result = self.graph.query(simple_query)\n",
    "                print(\"Sample nodes:\", result)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def find_matching_nodes(self, entity: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Find nodes in Neo4j graph that match the entity using Cypher queries\n",
    "\n",
    "        Args:\n",
    "            entity: Entity name to search for\n",
    "\n",
    "        Returns:\n",
    "            List of matching node names/identifiers\n",
    "        \"\"\"\n",
    "        entity_lower = entity.lower()\n",
    "        matching_nodes = set()\n",
    "\n",
    "        # Method 1: Use pre-built mapping\n",
    "        if entity_lower in self.node_mapping:\n",
    "            matching_nodes.update(self.node_mapping[entity_lower])\n",
    "\n",
    "        for node_key, node_list in self.node_mapping.items():\n",
    "            if entity_lower in node_key or node_key in entity_lower:\n",
    "                matching_nodes.update(node_list)\n",
    "\n",
    "        # Method 2: Direct Neo4j search (more flexible)\n",
    "        try:\n",
    "            # Search by name property\n",
    "            query1 = f\"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE toLower(n.name) CONTAINS toLower($entity)\n",
    "            RETURN n.name as name LIMIT 10\n",
    "            \"\"\"\n",
    "            result1 = self.graph.query(query1, params={\"entity\": entity})\n",
    "            for record in result1:\n",
    "                if record.get('name'):\n",
    "                    matching_nodes.add(record['name'])\n",
    "\n",
    "            # Search by labels\n",
    "            query2 = f\"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE any(label IN labels(n) WHERE toLower(label) CONTAINS toLower($entity))\n",
    "            RETURN n.name as name LIMIT 10\n",
    "            \"\"\"\n",
    "            result2 = self.graph.query(query2, params={\"entity\": entity})\n",
    "            for record in result2:\n",
    "                if record.get('name'):\n",
    "                    matching_nodes.add(record['name'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Neo4j search: {e}\")\n",
    "\n",
    "        return list(matching_nodes)\n",
    "\n",
    "    def k_hop_expansion(self, seed_nodes: List[str], k_hops: int = 2) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Expand k-hops from seed nodes using Neo4j\n",
    "\n",
    "        Args:\n",
    "            seed_nodes: Starting nodes\n",
    "            k_hops: Number of hops to expand\n",
    "\n",
    "        Returns:\n",
    "            Set of all nodes within k hops\n",
    "        \"\"\"\n",
    "        if not seed_nodes:\n",
    "            return set()\n",
    "\n",
    "        try:\n",
    "            # Use parameterized query to avoid string formatting issues\n",
    "            query = f\"\"\"\n",
    "            MATCH (start)\n",
    "            WHERE start.name IN $seed_names\n",
    "            CALL {{\n",
    "                WITH start\n",
    "                MATCH (start)-[*1..{k_hops}]-(connected)\n",
    "                RETURN connected.name as name\n",
    "            }}\n",
    "            RETURN DISTINCT name\n",
    "            \"\"\"\n",
    "\n",
    "            result = self.graph.query(query, params={\"seed_names\": seed_nodes})\n",
    "            expanded_nodes = set()\n",
    "\n",
    "            for record in result:\n",
    "                node_name = record.get('name')\n",
    "                if node_name:\n",
    "                    expanded_nodes.add(node_name)\n",
    "\n",
    "            # Add original seed nodes\n",
    "            expanded_nodes.update(seed_nodes)\n",
    "\n",
    "            return expanded_nodes\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in k-hop expansion: {e}\")\n",
    "            return set(seed_nodes)\n",
    "\n",
    "    def retrieve_subgraph_info(self, entities: List[str], k_hops: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main retrieval function: get relevant subgraph information from Neo4j\n",
    "\n",
    "        Args:\n",
    "            entities: List of extracted entities from query\n",
    "            k_hops: Number of hops to expand\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing subgraph info and metadata\n",
    "        \"\"\"\n",
    "        # Step 1: Find matching nodes for each entity\n",
    "        all_seed_nodes = []\n",
    "        entity_matches = {}\n",
    "\n",
    "        print(\"Searching for entity matches...\")\n",
    "        for entity in entities:\n",
    "            matching_nodes = self.find_matching_nodes(entity)\n",
    "            entity_matches[entity] = matching_nodes\n",
    "            all_seed_nodes.extend(matching_nodes)\n",
    "\n",
    "        print(f\"Entity matches found: {entity_matches}\")\n",
    "\n",
    "        if not all_seed_nodes:\n",
    "            return {\n",
    "                \"nodes\": [],\n",
    "                \"relationships\": [],\n",
    "                \"entity_matches\": entity_matches,\n",
    "                \"message\": \"No matching nodes found in graph\"\n",
    "            }\n",
    "\n",
    "        # Step 2: K-hop expansion\n",
    "        expanded_nodes = self.k_hop_expansion(all_seed_nodes, k_hops)\n",
    "        print(f\"Expanded to {len(expanded_nodes)} nodes after {k_hops} hops\")\n",
    "\n",
    "        # Step 3: Get subgraph with relationships\n",
    "        try:\n",
    "            # Use parameterized query to avoid string formatting issues\n",
    "            subgraph_query = \"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.name IN $node_names AND m.name IN $node_names\n",
    "            RETURN n.name as source, type(r) as relationship, m.name as target,\n",
    "                   n as source_node, m as target_node\n",
    "            \"\"\"\n",
    "\n",
    "            result = self.graph.query(subgraph_query, params={\"node_names\": list(expanded_nodes)})\n",
    "\n",
    "            relationships = []\n",
    "            nodes_info = {}\n",
    "\n",
    "            for record in result:\n",
    "                source = record.get('source')\n",
    "                target = record.get('target')\n",
    "                rel_type = record.get('relationship')\n",
    "\n",
    "                if source and target and rel_type:\n",
    "                    relationships.append({\n",
    "                        'source': source,\n",
    "                        'target': target,\n",
    "                        'relationship': rel_type\n",
    "                    })\n",
    "\n",
    "                # Store node information\n",
    "                if source:\n",
    "                    nodes_info[source] = record.get('source_node', {})\n",
    "                if target:\n",
    "                    nodes_info[target] = record.get('target_node', {})\n",
    "\n",
    "            return {\n",
    "                \"nodes\": list(expanded_nodes),\n",
    "                \"relationships\": relationships,\n",
    "                \"nodes_info\": nodes_info,\n",
    "                \"entity_matches\": entity_matches,\n",
    "                \"seed_nodes\": all_seed_nodes,\n",
    "                \"total_nodes\": len(expanded_nodes),\n",
    "                \"total_relationships\": len(relationships)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving subgraph: {e}\")\n",
    "            return {\n",
    "                \"nodes\": list(expanded_nodes),\n",
    "                \"relationships\": [],\n",
    "                \"entity_matches\": entity_matches,\n",
    "                \"seed_nodes\": all_seed_nodes,\n",
    "                \"total_nodes\": len(expanded_nodes),\n",
    "                \"total_relationships\": 0,\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183817c3-1707-4b32-acb1-b5f7ef10a5b9",
   "metadata": {},
   "source": [
    "Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc498f-ea11-4ca6-960b-96b89990a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AnswerResult:\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    knowledge_used: Dict[str, Any]\n",
    "    reasoning_steps: List[str]\n",
    "\n",
    "class GraphAnswerGenerator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the answer generator with Llama 3.3 via ChatGroq\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"qwen/qwen3-32b\",\n",
    "            temperature=0,  # Low temperature for factual accuracy\n",
    "        )\n",
    "\n",
    "    def format_graph_knowledge(self, retrieval_result) -> str:\n",
    "        \"\"\"\n",
    "        Convert graph retrieval results into structured text format\n",
    "\n",
    "        Args:\n",
    "            retrieval_result: Output from your Neo4jGraphRetriever (dict format)\n",
    "\n",
    "        Returns:\n",
    "            Formatted knowledge string\n",
    "        \"\"\"\n",
    "        knowledge_sections = []\n",
    "\n",
    "        # Section 1: Key Medical Entities Found\n",
    "        if retrieval_result.get('nodes'):\n",
    "            entities_text = \"MEDICAL ENTITIES:\\n\"\n",
    "            # Group nodes by likely type (you could enhance this with node properties)\n",
    "            for i, node in enumerate(retrieval_result['nodes'][:50], 1):  # Limit for context\n",
    "                entities_text += f\"- {node}\\n\"\n",
    "            knowledge_sections.append(entities_text)\n",
    "\n",
    "        # Section 2: Medical Relationships\n",
    "        if retrieval_result.get('relationships'):\n",
    "            relationships_text = \"MEDICAL RELATIONSHIPS:\\n\"\n",
    "            # Sort by importance if available\n",
    "            sorted_rels = sorted(\n",
    "                retrieval_result['relationships'],\n",
    "                key=lambda x: x.get('importance', 0.5),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            for rel in sorted_rels[:20]:  # Top 20 relationships\n",
    "                source = rel.get('source', 'Unknown')\n",
    "                target = rel.get('target', 'Unknown')\n",
    "                rel_type = rel.get('relationship', 'RELATED_TO')\n",
    "\n",
    "                # Convert relationship types to natural language\n",
    "                rel_description = self._relationship_to_text(rel_type)\n",
    "                relationships_text += f\"- {source} {rel_description} {target}\\n\"\n",
    "\n",
    "            knowledge_sections.append(relationships_text)\n",
    "\n",
    "        # Section 3: Entity Matching Context\n",
    "        if retrieval_result.get('entity_matches'):\n",
    "            matching_text = \"ENTITY CONTEXT:\\n\"\n",
    "            for entity, matched_nodes in retrieval_result['entity_matches'].items():\n",
    "                if matched_nodes:\n",
    "                    matching_text += f\"- Query term '{entity}' relates to: {', '.join(matched_nodes[:3])}\\n\"\n",
    "            knowledge_sections.append(matching_text)\n",
    "\n",
    "        # Section 4: Connection Paths (if available)\n",
    "        if retrieval_result.get('path_info') and retrieval_result['path_info'].get('paths'):\n",
    "            paths_text = \"KEY CONNECTIONS:\\n\"\n",
    "            for path in retrieval_result['path_info']['paths'][:5]:\n",
    "                start = path.get('start', '')\n",
    "                end = path.get('end', '')\n",
    "                relationships = path.get('relationships', [])\n",
    "                path_desc = ' → '.join(relationships) if relationships else 'connected to'\n",
    "                paths_text += f\"- {start} is {path_desc} {end}\\n\"\n",
    "            knowledge_sections.append(paths_text)\n",
    "\n",
    "        return \"\\n\".join(knowledge_sections)\n",
    "\n",
    "    def _relationship_to_text(self, rel_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Convertit un type de relation brut (Neo4j) en texte naturel :\n",
    "        - Met tout en minuscules\n",
    "        - Remplace les underscores '_' par des espaces\n",
    "        \"\"\"\n",
    "\n",
    "        return rel_type.lower().replace('_', ' ')\n",
    "\n",
    "    def create_answer_prompt(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Create the prompt template for answer generation\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a medical expert specializing in Type 2 Diabetes. Your role is to provide accurate, helpful answers based on the provided medical knowledge graph information.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY the provided medical knowledge\n",
    "2. Be precise , factual and explicative - avoid speculation\n",
    "3. Organize your answer logically (causes → mechanisms → treatments → outcomes)\n",
    "4. If the knowledge is insufficient, clearly state what information is missing\n",
    "5. Use medical terminology appropriately but explain complex concepts\n",
    "6. Focus on practical, clinically relevant information\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Direct answer first\n",
    "- Supporting details with mechanisms/relationships\n",
    "- Practical implications or recommendations\n",
    "- Note any limitations in the available knowledge\"\"\"),\n",
    "\n",
    "            (\"human\", \"\"\"Based on the following medical knowledge from a Type 2 Diabetes knowledge graph, please answer this question:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "AVAILABLE MEDICAL KNOWLEDGE:\n",
    "{formatted_knowledge}\n",
    "\n",
    "RETRIEVAL QUALITY: {confidence_score}/1.0\n",
    "REASONING CONTEXT: {retrieval_reasoning}\n",
    "\n",
    "Please provide a comprehensive answer based on this knowledge.\"\"\")\n",
    "        ])\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def generate_answer(self, question: str, retrieval_result) -> AnswerResult:\n",
    "        \"\"\"\n",
    "        Generate answer using structured knowledge from graph retrieval\n",
    "\n",
    "        Args:\n",
    "            question: Original user question\n",
    "            retrieval_result: Output from Neo4jGraphRetriever (dict format)\n",
    "\n",
    "        Returns:\n",
    "            AnswerResult with generated answer and metadata\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Format the knowledge\n",
    "        formatted_knowledge = self.format_graph_knowledge(retrieval_result)\n",
    "\n",
    "        # Step 2: Create the prompt\n",
    "        prompt = self.create_answer_prompt()\n",
    "\n",
    "        # Step 3: Prepare prompt variables\n",
    "        prompt_vars = {\n",
    "            \"question\": question,\n",
    "            \"formatted_knowledge\": formatted_knowledge,\n",
    "            \"confidence_score\": retrieval_result.get('subgraph_score', 0.5),\n",
    "            \"retrieval_reasoning\": retrieval_result.get('reasoning', 'Knowledge retrieved from graph database')\n",
    "        }\n",
    "\n",
    "        # Step 4: Generate answer\n",
    "        try:\n",
    "            chain = prompt | self.llm\n",
    "            response = chain.invoke(prompt_vars)\n",
    "            answer_text = response.content\n",
    "\n",
    "            # Step 5: Estimate answer confidence based on retrieval quality\n",
    "            answer_confidence = self._calculate_answer_confidence(\n",
    "                retrieval_result, formatted_knowledge, answer_text\n",
    "            )\n",
    "\n",
    "            # Step 6: Extract reasoning steps (basic implementation)\n",
    "            reasoning_steps = self._extract_reasoning_steps(answer_text)\n",
    "\n",
    "            return AnswerResult(\n",
    "                answer=answer_text,\n",
    "                confidence=answer_confidence,\n",
    "                knowledge_used={\n",
    "                    \"entities_count\": len(retrieval_result.get('nodes', [])),\n",
    "                    \"relationships_count\": len(retrieval_result.get('relationships', [])),\n",
    "                    \"retrieval_score\": retrieval_result.get('subgraph_score', 0.5),\n",
    "                    \"entity_matches\": len(retrieval_result.get('entity_matches', {}))\n",
    "                },\n",
    "                reasoning_steps=reasoning_steps\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return AnswerResult(\n",
    "                answer=f\"Error generating answer: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                knowledge_used={},\n",
    "                reasoning_steps=[\"Error occurred during generation\"]\n",
    "            )\n",
    "\n",
    "    def _calculate_answer_confidence(self, retrieval_result, formatted_knowledge: str, answer: str) -> float:\n",
    "        \"\"\"Calculate confidence score for the generated answer\"\"\"\n",
    "\n",
    "        # Factor 1: Retrieval quality\n",
    "        retrieval_confidence = retrieval_result.get('subgraph_score', 0.5)\n",
    "\n",
    "        # Factor 2: Knowledge richness\n",
    "        knowledge_richness = min(len(formatted_knowledge) / 1000, 1.0)  # Normalize to 0-1\n",
    "\n",
    "        # Factor 3: Answer completeness (basic heuristic)\n",
    "        answer_completeness = min(len(answer) / 500, 1.0) if len(answer) > 50 else 0.3\n",
    "\n",
    "        # Factor 4: Entity coverage\n",
    "        entity_matches = retrieval_result.get('entity_matches', {})\n",
    "        entities_covered = len(entity_matches) / max(len(entity_matches), 1)\n",
    "\n",
    "        # Weighted average\n",
    "        total_confidence = (\n",
    "            0.4 * retrieval_confidence +\n",
    "            0.3 * knowledge_richness +\n",
    "            0.2 * answer_completeness +\n",
    "            0.1 * entities_covered\n",
    "        )\n",
    "\n",
    "        return min(total_confidence, 1.0)\n",
    "\n",
    "    def _extract_reasoning_steps(self, answer: str) -> List[str]:\n",
    "        \"\"\"Extract basic reasoning steps from the answer\"\"\"\n",
    "        # Simple implementation - you could make this more sophisticated\n",
    "        sentences = answer.split('. ')\n",
    "        reasoning_steps = []\n",
    "\n",
    "        for sentence in sentences[:5]:  # Take first 5 sentences as reasoning steps\n",
    "            if len(sentence.strip()) > 20:\n",
    "                reasoning_steps.append(sentence.strip())\n",
    "\n",
    "        return reasoning_steps\n",
    "\n",
    "# ==================== COMPLETE PIPELINE TEST ====================\n",
    "\n",
    "def test_complete_pipeline(neo4j_graph):\n",
    "    \"\"\"Test the complete end-to-end pipeline\"\"\"\n",
    "\n",
    "    retriever = Neo4jGraphRetriever(graph)  \n",
    "    answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "    # Test question\n",
    "    test_question = \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\"\n",
    "\n",
    "    print(f\"QUESTION: {test_question}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Step 1: Extract entities (your existing code)\n",
    "    entity_result = entity_chain.invoke({\"question\": test_question})\n",
    "    entities = entity_result.names\n",
    "    print(f\"ENTITIES: {entities}\")\n",
    "\n",
    "    # Step 2: Retrieve relevant subgraph info\n",
    "    retrieval_result = retriever.retrieve_subgraph_info(entities, k_hops=2)\n",
    "\n",
    "    # Step 3: Generate answer\n",
    "    answer_result = answer_generator.generate_answer(test_question, retrieval_result)\n",
    "\n",
    "    print(f\"\\n GENERATED ANSWER:\")\n",
    "    print(\"-\"*50)\n",
    "    print(answer_result.answer)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    return answer_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc2d66-a9fc-4ce4-b7a2-140d1443d079",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91f821-4bda-4a0b-a70b-d5025a4e4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "retriever = Neo4jGraphRetriever(graph)\n",
    "answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "question = \"What are the first warning signs and symptoms of type 2 diabetes?\"\n",
    "\n",
    "# 1. Extract entities\n",
    "entities = entity_chain.invoke({\"question\": question}).names\n",
    "\n",
    "# 2. Retrieve graph knowledge\n",
    "retrieval_result = retriever.retrieve_subgraph_info(entities, k_hops=2)\n",
    "\n",
    "# 3. Generate answer\n",
    "answer_result = answer_generator.generate_answer(question, retrieval_result)\n",
    "\n",
    "print(f\"Answer: {answer_result.answer}\")\n",
    "print(f\"Confidence: {answer_result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe5ac4-8dac-4f2b-917e-89477966a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Neo4jGraphRetriever(graph)\n",
    "answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "question = \"What are the first warning signs and symptoms of type 2 diabetes?\"\n",
    "\n",
    "# 1. Extract entities\n",
    "entities = entity_chain.invoke({\"question\": question}).names\n",
    "\n",
    "# 2. Retrieve graph knowledge\n",
    "retrieval_result = retriever.retrieve_subgraph_info(entities, k_hops=2)\n",
    "\n",
    "# 3. Generate answer\n",
    "answer_result = answer_generator.generate_answer(question, retrieval_result)\n",
    "\n",
    "print(f\"Answer: {answer_result.answer}\")\n",
    "print(f\"Confidence: {answer_result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5e65b-e8b1-4224-89ca-c3248d8587ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Neo4jGraphRetriever(graph)\n",
    "answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "question = \"What are three ways to prevent type 2 diabetes? \"\n",
    "\n",
    "# 1. Extract entities\n",
    "entities = entity_chain.invoke({\"question\": question}).names\n",
    "\n",
    "# 2. Retrieve graph knowledge\n",
    "retrieval_result = retriever.retrieve_subgraph_info(entities, k_hops=2)\n",
    "\n",
    "# 3. Generate answer\n",
    "answer_result = answer_generator.generate_answer(question, retrieval_result)\n",
    "\n",
    "print(f\"Answer: {answer_result.answer}\")\n",
    "print(f\"Confidence: {answer_result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ed38b-bd30-43d6-8fb1-7cef0f282d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "737899ad-78c1-4075-ad8e-228fe99b6fad",
   "metadata": {},
   "source": [
    "<h4> Final Graph retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387524d-a854-4210-94f1-ac16bbeedabc",
   "metadata": {},
   "source": [
    "Find relevent relations to explore for every question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2e090-0e68-4910-83e3-a331ae36c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "from groq import Groq \n",
    "\n",
    "RELATIONS = [\n",
    "    \"ACTIVATED_BY\", \"ACTIVATES\", \"ACTS_ON\", \"ACTS_THROUGH\", \"ADVERSELY_AFFECTS\",\n",
    "    \"AFFECTED_BY\", \"AFFECTS\", \"ALLEVIATES\", \"ALTERNATIVE_TO\", \"ASSESSED_BY\",\n",
    "    \"ASSESSES\", \"ASSOCIATED_WITH\", \"ASSOCIATED_WITH_IMPROVED_MANAGEMENT\",\n",
    "    \"BARRIER_TO\", \"BARRIERS_TO\", \"BASED_ON\", \"BENEFICIAL_FOR\", \"BENEFITS\",\n",
    "    \"BENEFITS_FROM\", \"BINDS_TO\", \"BIOMARKER_FOR\", \"BIOMARKER_OF\", \"CAN_LEAD_TO\",\n",
    "    \"CAN_PROGRESS_TO\", \"CAUSED_BY\", \"CAUSES\", \"CHARACTERISTIC_OF\", \"CHARACTERIZED_BY\",\n",
    "    \"CHARACTERIZES\", \"CO_LOCALIZED_WITH\", \"CO_OCCURS_WITH\", \"CO-OCCURRING_WITH\",\n",
    "    \"COEXISTS_WITH\", \"COMBINATION_THERAPY_WITH\", \"COMORBID_WITH\", \"COMPARABLE_TO\",\n",
    "    \"COMPARED_TO\", \"COMPARED_WITH\", \"COMPARES_TO\", \"COMPLEMENTS\", \"COMPLICATED_BY\",\n",
    "    \"COMPLICATED_WITH\", \"COMPLICATES\", \"COMPLICATES_TREATMENT_OF\", \"COMPLICATION_OF\",\n",
    "    \"COMPONENT_OF\", \"COMPOSED_OF\", \"COMPRISES\", \"COMPROMISES\", \"CONSIDER\", \"CONTAINS\",\n",
    "    \"CONTRAINDICATED_WITH\", \"CONTRIBUTES_TO\", \"CONTROLS\", \"CORRELATED_WITH\",\n",
    "    \"COUNTERACTS\", \"DAMAGES\", \"DECREASES\", \"DECREASES_EXPRESSION_OF\", \"DECREASES_IN\",\n",
    "    \"DECREASES_RISK_OF\", \"DECREASES_WITH\", \"DEFINES\", \"DEGRADED_BY\", \"DEGRADES\",\n",
    "    \"DELIVERS\", \"DIAGNOSED_BY\", \"DIAGNOSES\", \"DIFFERENTIATED_FROM\", \"DISRUPTS\",\n",
    "    \"DISTINGUISHED_FROM\", \"DOES_NOT_AFFECT\", \"DOES_NOT_REDUCE_RISK_OF\",\n",
    "    \"DOES_NOT_SLOW_PROGRESSION_OF\", \"DOES_NOT_TREAT\", \"ENABLES\", \"ENCAPSULATES\",\n",
    "    \"ENHANCES\", \"ENHANCES_EFFECTIVENESS\", \"EQUIVALENT_TO\", \"EVALUATES\", \"EXPLAINS\",\n",
    "    \"EXPRESSED\", \"EXPRESSED_IN\", \"EXPRESSES\", \"FACILITATES\", \"FOCUSES_ON\", \"FUNDED\",\n",
    "    \"FUNDED_BY\", \"FUNDS\", \"GUIDES\", \"HAS\", \"HAS_ANABOLIC_EFFECT_ON\", \"HAS_BIOMARKER\",\n",
    "    \"HAS_COMORBIDITY\", \"HAS_COMPLICATION\", \"HAS_COMPONENT\", \"HAS_EFFECT_ON\",\n",
    "    \"HAS_IMMUNOMODULATORY_EFFECTS_ON\", \"HAS_IMPACT_ON\", \"HAS_NEGATIVE_EFFECT_ON\",\n",
    "    \"HAS_NO_EFFECT\", \"HAS_NO_EFFECT_ON\", \"HAS_NO_INFERIOR_RISK_OF\",\n",
    "    \"HAS_POSITIVE_EFFECT_ON\", \"HAS_SIDE_EFFECT\", \"HAS_SUBTYPE\", \"HAS_SYMPTOM\",\n",
    "    \"HINDERS\", \"IMPAIRS\", \"IMPLEMENTS\", \"IMPORTANT_FOR\", \"IMPROVED_BY\", \"IMPROVES\",\n",
    "    \"IMPROVES_OUTCOME\", \"IMPROVES_OUTCOME_OF\", \"IMPROVES_OUTCOMES\", \"INCLUDES\",\n",
    "    \"INCREASED_BY\", \"INCREASES\", \"INCREASES_ABUNDANCE_OF\", \"INCREASES_ACTIVITY_OF\",\n",
    "    \"INCREASES_ADHERENCE_TO\", \"INCREASES_EXPRESSION_OF\", \"INCREASES_LEVEL_OF\",\n",
    "    \"INCREASES_PRODUCTION_OF\", \"INCREASES_RISK_OF\", \"INCREASES_SECRETION_OF\",\n",
    "    \"INCREASES_UTILIZATION_OF\", \"INCREASES_WITH\", \"INDICATES\", \"INDICATIVE_OF\",\n",
    "    \"INDICATOR_OF\", \"INDUCES\", \"INFECTS\", \"INFLUENCED_BY\", \"INFLUENCES\", \"INFORMS\",\n",
    "    \"INHIBITED_BY\", \"INHIBITS\", \"INTERACTS_WITH\", \"INVERSELY_ASSOCIATED_WITH\",\n",
    "    \"INVERSELY_CORRELATED_WITH\", \"INVERSELY_RELATED_TO\", \"INVOLVED_IN\", \"IS\", \"IS_A\",\n",
    "    \"IS_COMPLICATION_OF\", \"IS_TYPE_OF\", \"LEADS_TO\", \"LINKED_TO\", \"LOCATED_IN\",\n",
    "    \"MAINTAINS\", \"MANAGED_BY\", \"MANAGED_WITH\", \"MANAGES\", \"MANUFACTURES\",\n",
    "    \"MARKER_OF\", \"MAY_AFFECT\", \"MAY_CAUSE\", \"MAY_REDUCE_RISK_OF\", \"MEASURED_AS\",\n",
    "    \"MEASURED_BY\", \"MEASURES\", \"MEDIATES\", \"MENTIONED_IN\", \"METABOLIC_PRODUCT_OF\",\n",
    "    \"MIMICS\", \"MODEL_OF\", \"MODELS\", \"MODERATES\", \"MODIFIES\", \"MODULATES\", \"MONITORS\",\n",
    "    \"NEGATIVELY_ASSOCIATED_WITH\", \"NEGATIVELY_CORRELATED_WITH\", \"NEGATIVELY_IMPACT\",\n",
    "    \"NO_EFFECT_ON\", \"NOT_ASSOCIATED_WITH\", \"PART_OF\", \"PHOSPHORYLATES\",\n",
    "    \"POSITIVELY_ASSOCIATED_WITH\", \"POSITIVELY_CORRELATED_WITH\", \"PREDICTIVE_OF\",\n",
    "    \"PREDICTS\", \"PRESERVES\", \"PREVENTS\", \"PREVENTS_COMPLICATIONS\", \"PRODUCED_BY\",\n",
    "    \"PRODUCES\", \"PROGRESSES_TO\", \"PROMOTES\", \"PROTECTIVE_AGAINST\", \"PROTECTS\",\n",
    "    \"PROTECTS_AGAINST\", \"PROVIDES\", \"RECOMMENDED_FOR\", \"RECOMMENDS\", \"REDUCES\",\n",
    "    \"REDUCES_EXPRESSION_OF\", \"REDUCES_RISK_OF\", \"REDUCES_SEVERITY_OF\",\n",
    "    \"REDUCES_SYMPTOMS_OF\", \"REGULATED_BY\", \"REGULATES\", \"RELATED_TO\", \"REPLACES\",\n",
    "    \"REQUIRES\", \"RESEARCHES\", \"RESPONDS_TO\", \"RISK_FACTOR_FOR\", \"SIDE_EFFECT_OF\",\n",
    "    \"SIMILAR_TO\", \"SLOWS_PROGRESSION_OF\", \"STIMULATES\", \"SUBTYPE_OF\", \"SUPPORT\",\n",
    "    \"SUPPORTED\", \"SUPPORTED_BY\", \"SUPPORTS\", \"SUPPRESSES\", \"TARGETS\", \"TESTED_IN\",\n",
    "    \"TREATED_WITH\", \"TREATS\", \"TREATS_WITH\", \"TRIGGERS\", \"USED_FOR\",\n",
    "    \"USED_FOR_DIAGNOSIS_OF\", \"USED_IN\", \"USED_IN_COMBINATION_WITH\", \"USED_TO_ASSESS\",\n",
    "    \"USED_TO_DERIVE\", \"USED_TO_DIAGNOSE\", \"USED_TO_MONITOR\", \"USED_TO_PREDICT\",\n",
    "    \"USED_TO_UNDERSTAND\", \"USES\", \"WORSENS\"\n",
    "]\n",
    "\n",
    "def classify_relations(question: str, relations: List[str]) -> Dict[str, List[str]]:\n",
    "    client = Groq(api_key=\"api_key\")\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert in medical knowledge graphs for type 2 diabetes. \"\n",
    "        \"Given a question and a large list of biomedical relations, \"\n",
    "        \"identify:\\n\"\n",
    "        \"- PRIMARY_RELATIONS: Essential relations to answer the question directly.\\n\"\n",
    "        \"- SECONDARY_RELATIONS: Useful but non-essential.\\n\"\n",
    "        \"Output strictly in JSON with the format:\\n\"\n",
    "        \"{'primary_relations': [...], 'secondary_relations': [...]}.\"\n",
    "    )\n",
    "\n",
    "    examples = [\n",
    "        {\n",
    "            \"question\": \"How does obesity lead to insulin resistance?\",\n",
    "            \"answer\": {\n",
    "                \"primary_relations\": [\"CAUSES\",\"CAUSED_BY\",\"LEADS_TO\",\"CONTRIBUTES_TO\",\"PROMOTES\",\"INDUCES\",\"INCREASES_RISK_OF\",\"INVOLVED_IN\"],\n",
    "                \"secondary_relations\": [\"ASSOCIATED_WITH\",\"LINKED_TO\",\"CORRELATED_WITH\",\"INFLUENCES\",\"AFFECTS\",\"RELATED_TO\",\"RISK_FACTOR_FOR\",\"INCREASES\",\"MEDIATES\",\"MODULATES\",\"TRIGGERS\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the mechanisms by which GLP-1 receptor agonists improve cardiovascular outcomes in type 2 diabetes?\",\n",
    "            \"answer\": {\n",
    "                \"primary_relations\": [\"IMPROVES_OUTCOME\",\"IMPROVES_OUTCOMES\",\"IMPROVES_OUTCOME_OF\",\"BENEFICIAL_FOR\",\"REDUCES_RISK_OF\",\"PROTECTS\",\"PROTECTS_AGAINST\",\"ENHANCES_EFFECTIVENESS\",\"MECHANISM_OF_ACTION\",\"MEDIATES\",\"MODULATES\"],\n",
    "                \"secondary_relations\": [\"ASSOCIATED_WITH\",\"LINKED_TO\",\"AFFECTS\",\"HAS_POSITIVE_EFFECT_ON\",\"INFLUENCES\",\"INVOLVED_IN\",\"SUPPORTS\",\"PROMOTES\",\"CONTRIBUTES_TO\",\"ENHANCES\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"what are the symptoms of type 2 diabetes?\",\n",
    "            \"answer\": {\n",
    "                \"primary_relations\": [\"HAS_SYMPTOM\",\"CHARACTERIZED_BY\",\"INDICATES\",\"INDICATIVE_OF\",\"MARKER_OF\"],\n",
    "                \"secondary_relations\": [\"ASSOCIATED_WITH\",\"LINKED_TO\",\"RELATED_TO\",\"PREDICTIVE_OF\",\"SUGGESTS\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    few_shot_prompt = \"Examples:\\n\"\n",
    "    for ex in examples:\n",
    "        few_shot_prompt += f\"Q: {ex['question']}\\nA: {json.dumps(ex['answer'], indent=2)}\\n\\n\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    {few_shot_prompt}\n",
    "    Question: {question}\n",
    "    Predefined relations: {relations}\n",
    "\n",
    "    Return ONLY JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    output_text = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        parsed = json.loads(output_text.replace(\"'\", '\"'))\n",
    "        return parsed\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ JSON parsing failed:\", output_text)\n",
    "        return {\"primary_relations\": [], \"secondary_relations\": []}\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "q1 = \"What factors contribute to the development of diabetic neuropathy in patients with type 2 diabetes?\"\n",
    "print(classify_relations(q1, RELATIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f3dfe-65b9-41e9-b9a7-765be4514701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq  \n",
    "\n",
    "# Define the structured output schema\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All medical relevant entities related to Type 2 Diabetes\",\n",
    "    )\n",
    "\n",
    "# Initialize your LLM \n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",   \n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Few-shot prompt with examples\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a medical expert specializing in Type 2 Diabetes.\"\n",
    "         \"Your task is to extract structured knowledge from biomedical questions. \"\n",
    "         \"Always extract all relevant entities from the text. \"\n",
    "        ),\n",
    "\n",
    "        # --- FEW-SHOT EXAMPLES ---\n",
    "        (\"human\", \"Extract structured knowledge about Type 2 Diabetes from the following question: \"\n",
    "                  \"How do epigenetic modifications and gut microbiome dysbiosis interact to influence insulin resistance and β-cell dysfunction in the progression of Type 2 Diabetes?\"),\n",
    "        (\"ai\", '{{\"names\": [\"epigenetic modifications\", \"gut microbiome dysbiosis\", \"insulin resistance\",\"β-cell dysfunction\"]}}'),\n",
    "\n",
    "        (\"human\", \"Extract structured knowledge about Type 2 Diabetes from the following question: \"\n",
    "                  \"What are common complications of Type 2 Diabetes?\"),\n",
    "        (\"ai\", '{{\"names\": [\"Type 2 Diabetes\"]}}'),\n",
    "\n",
    "        # --- ACTUAL QUESTION ---\n",
    "        (\"human\", \"Extract structured knowledge about Type 2 Diabetes from the following question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create chain with structured output\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "\n",
    "# test question\n",
    "result = entity_chain.invoke({\"question\": \"Why might individuals with central obesity have a higher likelihood of developing complications related to Type 2 Diabetes?\"})\n",
    "print(\"Diagnostic tests:\", result.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b1e43-9fc4-4b10-8fa3-87eb0f38426b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1dfe0-f1b3-42da-ade1-e7e62b18c41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd3d71-9e71-48eb-a3dd-aaf5fbf3d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict, Any, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class EntityMatch:\n",
    "    entity: str\n",
    "    matched_nodes: List[str]\n",
    "    match_scores: List[float]\n",
    "    match_types: List[str]  # 'exact', 'partial', 'semantic'\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    nodes: List[str]\n",
    "    relationships: List[Dict[str, Any]]\n",
    "    entity_matches: List[EntityMatch]\n",
    "    subgraph_score: float\n",
    "    path_info: Dict[str, Any]\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class GraphRetriever:\n",
    "    def __init__(self, neo4j_graph, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Neo4j graph retriever intégré avec extraction d'entités et classification de relations\n",
    "\n",
    "        Args:\n",
    "            neo4j_graph: Votre objet Neo4j graph\n",
    "            embedding_model: Modèle d'embedding optionnel pour similarité sémantique\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self.embedding_model = embedding_model\n",
    "        self.node_cache = {}\n",
    "        self.relationship_cache = {}\n",
    "\n",
    "    def _get_node_properties(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get all available node properties to search across\"\"\"\n",
    "        query = \"\"\"\n",
    "        CALL db.schema.nodeTypeProperties()\n",
    "        YIELD nodeType, propertyName, propertyTypes\n",
    "        RETURN nodeType, collect(propertyName) as properties\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.graph.query(query)\n",
    "            prop_mapping = {}\n",
    "            for record in result:\n",
    "                node_type = record.get('nodeType')\n",
    "                properties = record.get('properties', [])\n",
    "                prop_mapping[node_type] = properties\n",
    "            return prop_mapping\n",
    "        except:\n",
    "            # Fallback: assume common properties\n",
    "            return {\"*\": [\"name\", \"title\", \"description\", \"type\", \"label\"]}\n",
    "\n",
    "    def find_matching_nodes_advanced(self, entity: str) -> EntityMatch:\n",
    "        \"\"\"\n",
    "        Advanced node matching avec amélioration de la stratégie de matching\n",
    "        \"\"\"\n",
    "        matched_nodes = []\n",
    "        match_scores = []\n",
    "        match_types = []\n",
    "\n",
    "        # PREPROCESSING: Clean and tokenize the entity\n",
    "        entity_clean = entity.lower().strip()\n",
    "        entity_tokens = entity_clean.replace('-', ' ').replace('_', ' ').split()\n",
    "\n",
    "        # Strategy 1: Exact name matching\n",
    "        exact_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE toLower(n.name) = toLower($entity)\n",
    "        RETURN n.name as name, labels(n) as labels, 1.0 as score\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.graph.query(exact_query, params={\"entity\": entity})\n",
    "            for record in result:\n",
    "                if record.get('name'):\n",
    "                    matched_nodes.append(record['name'])\n",
    "                    match_scores.append(1.0)\n",
    "                    match_types.append('exact')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in exact matching: {e}\")\n",
    "\n",
    "        # Strategy 2: TOKEN-BASED MATCHING \n",
    "        token_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.name IS NOT NULL\n",
    "        WITH n, toLower(n.name) as node_name_lower\n",
    "        WHERE any(token IN $entity_tokens WHERE node_name_lower CONTAINS token AND size(token) > 2)\n",
    "          OR any(token IN $entity_tokens WHERE node_name_lower = token)\n",
    "        RETURN n.name as name,\n",
    "              CASE\n",
    "                  WHEN any(token IN $entity_tokens WHERE toLower(n.name) = token) THEN 0.9\n",
    "                  WHEN any(token IN $entity_tokens WHERE toLower(n.name) CONTAINS token AND size(token) > 3) THEN 0.8\n",
    "                  WHEN any(token IN $entity_tokens WHERE toLower(n.name) CONTAINS token) THEN 0.7\n",
    "                  ELSE 0.5\n",
    "              END as score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # FIX: Filtrage des tokens non pertinents\n",
    "            stopwords = {'levels', 'with', 'in', 'of', 'the', 'and', 'or'}\n",
    "            meaningful_tokens = [t for t in entity_tokens if t not in stopwords and len(t) > 2]\n",
    "\n",
    "            if meaningful_tokens:\n",
    "                result = self.graph.query(token_query, params={\"entity_tokens\": meaningful_tokens})\n",
    "                for record in result:\n",
    "                    name = record.get('name')\n",
    "                    if name and name not in matched_nodes:\n",
    "                        matched_nodes.append(name)\n",
    "                        match_scores.append(record.get('score', 0.75))\n",
    "                        match_types.append('token_match')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in token matching: {e}\")\n",
    "\n",
    "        # Strategy 3: CORE TERM EXTRACTION \n",
    "        core_terms = []\n",
    "        medical_patterns = {\n",
    "            'hba1c': ['hba1c', 'hemoglobin a1c', 'glycated hemoglobin', 'a1c'],\n",
    "            'blood pressure': ['blood pressure', 'bp', 'hypertension', 'systolic', 'diastolic'],\n",
    "            'blood sugar': ['blood sugar', 'glucose', 'glycemia', 'blood glucose'],\n",
    "            'cholesterol': ['cholesterol', 'ldl', 'hdl', 'lipids'],\n",
    "            'insulin': ['insulin', 'insulin resistance', 'insulin sensitivity'],\n",
    "            'obesity': ['obesity', 'bmi', 'body mass index', 'weight'],\n",
    "            'neuropathy': ['neuropathy', 'nerve damage', 'diabetic neuropathy']\n",
    "        }\n",
    "\n",
    "        for pattern, variants in medical_patterns.items():\n",
    "            if any(variant in entity_clean for variant in variants):\n",
    "                core_terms.append(pattern)\n",
    "\n",
    "        if core_terms:\n",
    "            core_query = \"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE any(term IN $core_terms WHERE toLower(n.name) CONTAINS term)\n",
    "            RETURN n.name as name, 0.9 as score\n",
    "            LIMIT 5\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                result = self.graph.query(core_query, params={\"core_terms\": core_terms})\n",
    "                for record in result:\n",
    "                    name = record.get('name')\n",
    "                    if name and name not in matched_nodes:\n",
    "                        matched_nodes.append(name)\n",
    "                        match_scores.append(0.9)\n",
    "                        match_types.append('core_term')\n",
    "            except Exception as e:\n",
    "                print(f\"Error in core term matching: {e}\")\n",
    "\n",
    "        # Strategy 4: Partial text matching\n",
    "        partial_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE toLower(n.name) CONTAINS toLower($entity)\n",
    "           OR toLower(coalesce(n.description, '')) CONTAINS toLower($entity)\n",
    "           OR any(label IN labels(n) WHERE toLower(label) CONTAINS toLower($entity))\n",
    "        RETURN n.name as name,\n",
    "               CASE\n",
    "                   WHEN toLower(n.name) CONTAINS toLower($entity) THEN 0.8\n",
    "                   WHEN toLower(coalesce(n.description, '')) CONTAINS toLower($entity) THEN 0.5\n",
    "                   ELSE 0.3\n",
    "               END as score,\n",
    "               labels(n) as labels\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.graph.query(partial_query, params={\"entity\": entity})\n",
    "            for record in result:\n",
    "                name = record.get('name')\n",
    "                if name and name not in matched_nodes:\n",
    "                    matched_nodes.append(name)\n",
    "                    match_scores.append(record.get('score', 0.5))\n",
    "                    match_types.append('partial')\n",
    "        except Exception as e:\n",
    "            print(f\"Error in partial matching: {e}\")\n",
    "\n",
    "        return EntityMatch(\n",
    "            entity=entity,\n",
    "            matched_nodes=matched_nodes,\n",
    "            match_scores=match_scores,\n",
    "            match_types=match_types\n",
    "        )\n",
    "\n",
    "    def adaptive_hop_expansion(\n",
    "        self,\n",
    "        seed_nodes: List[str],\n",
    "        entities: List[str],\n",
    "        primary_relations: List[str],\n",
    "        secondary_relations: List[str]\n",
    "    ) -> Tuple[Set[str], str]:\n",
    "        \"\"\"\n",
    "        Expansion adaptative utilisant les relations identifiées par le classificateur LLM\n",
    "        \"\"\"\n",
    "        if not seed_nodes:\n",
    "            return set(), \"No seed nodes found\"\n",
    "\n",
    "        # Analyse densité autour des seed nodes\n",
    "        density_query = \"\"\"\n",
    "        MATCH (start)-[r]-(connected)\n",
    "        WHERE start.name IN $seed_nodes\n",
    "        RETURN start.name AS node, count(r) AS degree\n",
    "        \"\"\"\n",
    "\n",
    "        node_degrees = {}\n",
    "        try:\n",
    "            result = self.graph.query(density_query, {\"seed_nodes\": seed_nodes})\n",
    "            for record in result:\n",
    "                node_degrees[record.get(\"node\")] = record.get(\"degree\", 0)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Density query failed - {e}\")\n",
    "\n",
    "        avg_degree = np.mean(list(node_degrees.values())) if node_degrees else 5\n",
    "\n",
    "        # Choix dynamique du nombre de hops\n",
    "        if avg_degree > 20:\n",
    "            k_hops = 1\n",
    "            reasoning = f\"High graph density ({avg_degree:.1f}) → 1 hop with strong relation filtering\"\n",
    "        elif avg_degree > 10:\n",
    "            k_hops = 2\n",
    "            reasoning = f\"Medium density ({avg_degree:.1f}) → 2 hops with mixed filtering\"\n",
    "        else:\n",
    "            k_hops = 3\n",
    "            reasoning = f\"Low density ({avg_degree:.1f}) → 3 hops with broader filtering\"\n",
    "\n",
    "        # Pondération dynamique basée sur primary/secondary\n",
    "        def get_relation_score(rel_type: str) -> float:\n",
    "            if rel_type in primary_relations:\n",
    "                return 0.9\n",
    "            elif rel_type in secondary_relations:\n",
    "                return 0.8\n",
    "            else:\n",
    "                return 0.5\n",
    "\n",
    "        expanded_nodes: Set[str] = set(seed_nodes)\n",
    "        current_nodes = set(seed_nodes)\n",
    "\n",
    "        for hop in range(k_hops):\n",
    "            if not current_nodes:\n",
    "                break\n",
    "\n",
    "            expansion_query = \"\"\"\n",
    "            MATCH (current)-[r]-(next)\n",
    "            WHERE current.name IN $current_nodes\n",
    "            RETURN DISTINCT next.name AS name, type(r) AS rel_type\n",
    "            \"\"\"\n",
    "\n",
    "            new_nodes = set()\n",
    "            try:\n",
    "                result = self.graph.query(expansion_query, {\"current_nodes\": list(current_nodes)})\n",
    "                for record in result:\n",
    "                    rel_type = record.get(\"rel_type\")\n",
    "                    score = get_relation_score(rel_type)\n",
    "                    node_name = record.get(\"name\")\n",
    "\n",
    "                    if score >= 0.6 and node_name not in expanded_nodes:\n",
    "                        new_nodes.add(node_name)\n",
    "                        expanded_nodes.add(node_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Expansion error at hop {hop}: {e}\")\n",
    "                break\n",
    "\n",
    "            current_nodes = new_nodes\n",
    "\n",
    "        return expanded_nodes, reasoning\n",
    "\n",
    "    def find_connecting_paths(self, nodes: List[str], primary_relations: List[str],\n",
    "                              secondary_relations: List[str], max_length: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        FIX: Correction de la requête de chemins\n",
    "        \"\"\"\n",
    "        if len(nodes) < 2:\n",
    "            return []\n",
    "\n",
    "        paths = []\n",
    "\n",
    "        # FIX: Construction correcte des conditions CASE\n",
    "        primary_conditions = [f\"type(r) = '{rel}'\" for rel in primary_relations]\n",
    "        secondary_conditions = [f\"type(r) = '{rel}'\" for rel in secondary_relations]\n",
    "\n",
    "        primary_case = \" OR \".join(primary_conditions) if primary_conditions else \"false\"\n",
    "        secondary_case = \" OR \".join(secondary_conditions) if secondary_conditions else \"false\"\n",
    "\n",
    "        path_query = f\"\"\"\n",
    "        MATCH path = (start)-[*1..{max_length}]-(end)\n",
    "        WHERE start.name IN $nodes AND end.name IN $nodes AND start.name <> end.name\n",
    "        WITH path, start.name as start_name, end.name as end_name, length(path) as path_length,\n",
    "             [r in relationships(path) | type(r)] as rel_types\n",
    "        RETURN start_name, end_name, path_length, rel_types,\n",
    "               reduce(score = 1.0, r in relationships(path) |\n",
    "                   score * CASE\n",
    "                       WHEN ({primary_case}) THEN 0.9\n",
    "                       WHEN ({secondary_case}) THEN 0.8\n",
    "                       ELSE 0.6\n",
    "                   END\n",
    "               ) as path_score\n",
    "        ORDER BY path_score DESC, path_length ASC\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.graph.query(path_query, params={\"nodes\": nodes})\n",
    "            for record in result:\n",
    "                paths.append({\n",
    "                    'start': record.get('start_name'),\n",
    "                    'end': record.get('end_name'),\n",
    "                    'length': record.get('path_length'),\n",
    "                    'relationships': record.get('rel_types', []),\n",
    "                    'score': record.get('path_score', 0.5)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding paths: {e}\")\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def calculate_subgraph_relevance(self, nodes: List[str], relationships: List[Dict],\n",
    "                                   entity_matches: List[EntityMatch]) -> float:\n",
    "        \"\"\"Calculate relevance score for the retrieved subgraph\"\"\"\n",
    "        # Factor 1: Entity match quality\n",
    "        match_score = 0.0\n",
    "        if entity_matches:\n",
    "            total_matches = sum(len(em.matched_nodes) for em in entity_matches)\n",
    "            if total_matches > 0:\n",
    "                weighted_score = sum(\n",
    "                    max(em.match_scores) * len(em.matched_nodes)\n",
    "                    for em in entity_matches if em.match_scores\n",
    "                )\n",
    "                match_score = weighted_score / total_matches\n",
    "\n",
    "        # Factor 2: Graph connectivity\n",
    "        connectivity_score = 0.0\n",
    "        if len(nodes) > 1 and relationships:\n",
    "            connectivity_score = math.log1p(len(relationships)) / math.log1p(len(nodes)**2)\n",
    "\n",
    "        # Factor 3: Subgraph size appropriateness\n",
    "        size_score = 0.0\n",
    "        if 3 <= len(nodes) <= 50:\n",
    "            size_score = 1.0\n",
    "        elif len(nodes) < 3:\n",
    "            size_score = len(nodes) / 3.0\n",
    "        else:\n",
    "            size_score = max(0.1, 50.0 / len(nodes))\n",
    "\n",
    "        # Weighted combination\n",
    "        total_score = (0.6 * match_score + 0.3 * connectivity_score + 0.1 * size_score)\n",
    "        return min(total_score, 1.0)\n",
    "\n",
    "    def retrieve_subgraph_advanced(self, entities: List[str], question: str,\n",
    "                                 min_relevance: float = 0.3) -> RetrievalResult:\n",
    "        \"\"\"\n",
    "        FIX: Méthode principale intégrant les trois composants\n",
    "        \"\"\"\n",
    "        # Étape 1: Classification des relations basées sur la question\n",
    "        relation_classification = classify_relations(question, RELATIONS)\n",
    "        primary_relations = relation_classification.get(\"primary_relations\", [])\n",
    "        secondary_relations = relation_classification.get(\"secondary_relations\", [])\n",
    "\n",
    "        print(f\"[DEBUG] Primary Relations: {primary_relations}\")\n",
    "        print(f\"[DEBUG] Secondary Relations: {secondary_relations}\")\n",
    "\n",
    "        # Étape 2: Matching des entités\n",
    "        entity_matches, all_seed_nodes = [], []\n",
    "        for entity in entities:\n",
    "            match_result = self.find_matching_nodes_advanced(entity)\n",
    "            entity_matches.append(match_result)\n",
    "            all_seed_nodes.extend(match_result.matched_nodes[:5])  # Top 5 matches par entité\n",
    "\n",
    "        if not all_seed_nodes:\n",
    "            return RetrievalResult([], [], entity_matches, 0.0, {},\n",
    "                                 \"No matching nodes found in graph\")\n",
    "\n",
    "        # Étape 3: Recherche des chemins connectants\n",
    "        path_info = {\n",
    "            'paths': self.find_connecting_paths(all_seed_nodes, primary_relations,\n",
    "                                              secondary_relations, max_length=3),\n",
    "            'seed_nodes': all_seed_nodes\n",
    "        }\n",
    "\n",
    "        # Étape 4: Expansion adaptative autour des seeds\n",
    "        expanded_nodes, expansion_reasoning = self.adaptive_hop_expansion(\n",
    "            all_seed_nodes, entities, primary_relations, secondary_relations\n",
    "        )\n",
    "\n",
    "        # Étape 5: Récupération des relations du sous-graphe\n",
    "        relationships = []\n",
    "        if len(expanded_nodes) > 1:\n",
    "            subgraph_query = \"\"\"\n",
    "            MATCH (n)-[r]-(m)\n",
    "            WHERE n.name IN $expanded_nodes AND m.name IN $expanded_nodes\n",
    "            RETURN DISTINCT n.name as source, type(r) as relationship, m.name as target,\n",
    "                  CASE\n",
    "                      WHEN type(r) IN $primary_relations THEN 1.0\n",
    "                      WHEN type(r) IN $secondary_relations THEN 0.9\n",
    "                      ELSE 0.7\n",
    "                  END as rel_importance\n",
    "            ORDER BY rel_importance DESC\n",
    "            LIMIT 100\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                result = self.graph.query(subgraph_query, params={\n",
    "                    \"expanded_nodes\": list(expanded_nodes),\n",
    "                    \"primary_relations\": primary_relations,\n",
    "                    \"secondary_relations\": secondary_relations\n",
    "                })\n",
    "                for record in result:\n",
    "                    relationships.append({\n",
    "                        'source': record.get('source'),\n",
    "                        'target': record.get('target'),\n",
    "                        'relationship': record.get('relationship'),\n",
    "                        'importance': record.get('rel_importance', 0.5)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting relationships: {e}\")\n",
    "\n",
    "        # Étape 6: Scoring final du sous-graphe\n",
    "        subgraph_score = self.calculate_subgraph_relevance(list(expanded_nodes),\n",
    "                                                         relationships, entity_matches)\n",
    "\n",
    "        reasoning = (f\"Good relevance ({subgraph_score:.2f}). {expansion_reasoning}\"\n",
    "                    if subgraph_score >= min_relevance\n",
    "                    else f\"Low relevance ({subgraph_score:.2f}). {expansion_reasoning}\")\n",
    "\n",
    "        return RetrievalResult(\n",
    "            nodes=list(expanded_nodes),\n",
    "            relationships=relationships,\n",
    "            entity_matches=entity_matches,\n",
    "            subgraph_score=subgraph_score,\n",
    "            path_info=path_info,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    # FIX: Méthode d'interface principale pour intégrer avec les autres composants\n",
    "    def retrieve_for_question(self, question: str, min_relevance: float = 0.3) -> RetrievalResult:\n",
    "        \"\"\"\n",
    "        Méthode principale qui intègre extraction d'entités + classification de relations + retrieval\n",
    "\n",
    "        Args:\n",
    "            question: Question de l'utilisateur\n",
    "            min_relevance: Score minimal de pertinence\n",
    "\n",
    "        Returns:\n",
    "            RetrievalResult avec le sous-graphe pertinent\n",
    "        \"\"\"\n",
    "        # Étape 1: Extraction des entités depuis la question\n",
    "\n",
    "        # Pour l'instant, utilisation directe:\n",
    "        entities = entity_chain.invoke({\"question\": question}).names\n",
    "\n",
    "        # Étape 2: Retrieval avec la méthode avancée\n",
    "        return self.retrieve_subgraph_advanced(entities, question, min_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa92c3-7665-4460-ab05-9013c6176ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== COMPLETE TEST PIPELINE ====================\n",
    "\n",
    "# Step 1: Initialize both components\n",
    "retriever = GraphRetriever(graph)\n",
    "\n",
    "# Step 2: Test with real medical questions\n",
    "test_questions = [\n",
    "    \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\"\n",
    "]\n",
    "\n",
    "# Step 3: Run complete pipeline\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n TEST {i}: {question}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # ENTITY EXTRACTION (Your existing code)\n",
    "    entity_result = entity_chain.invoke({\"question\": question})\n",
    "    extracted_entities = entity_result.names\n",
    "    print(f\"📋 Extracted Entities: {extracted_entities}\")\n",
    "\n",
    "    # GRAPH RETRIEVAL (Improved retriever)\n",
    "    retrieval_result = retriever.retrieve_subgraph_advanced(extracted_entities, question)\n",
    "\n",
    "    # RESULTS ANALYSIS\n",
    "    print(f\"RETRIEVAL RESULTS:\")\n",
    "    print(f\"   • Relevance Score: {retrieval_result.subgraph_score:.2f}/1.0\")\n",
    "    print(f\"   • Strategy: {retrieval_result.reasoning}\")\n",
    "    print(f\"   • Nodes Retrieved: {len(retrieval_result.nodes)}\")\n",
    "    print(f\"   • Relationships Found: {len(retrieval_result.relationships)}\")\n",
    "\n",
    "    print(f\"\\n ENTITY MATCHING QUALITY:\")\n",
    "    for match in retrieval_result.entity_matches:\n",
    "        print(f\"   '{match.entity}' → {len(match.matched_nodes)} matches\")\n",
    "        for node, score, match_type in zip(match.matched_nodes[:5], match.match_scores[:5], match.match_types[:5]):\n",
    "            print(f\"      {node} (score: {score:.2f}, type: {match_type})\")\n",
    "\n",
    "    print(f\"\\n KEY RELATIONSHIPS:\")\n",
    "    top_rels = sorted(retrieval_result.relationships, key=lambda x: x.get('importance', 0), reverse=True)[:10]\n",
    "    for rel in top_rels:\n",
    "        print(f\"   {rel['source']} --[{rel['relationship']}]--> {rel['target']}\")\n",
    "\n",
    "    if retrieval_result.path_info['paths']:\n",
    "        print(f\"\\n DISCOVERED PATHS:\")\n",
    "        for path in retrieval_result.path_info['paths'][:5]:\n",
    "            print(f\"   {path['start']} → {path['end']} (length: {path['length']}, score: {path['score']:.2f})\")\n",
    "\n",
    "    print(f\"\\n✅ READY FOR ANSWER GENERATION: {retrieval_result.subgraph_score >= 0.5}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80112f-f1d2-47b9-94e6-a54d544a1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb2b39-983e-4eef-b99f-52dc864c9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AnswerResult:\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    knowledge_used: Dict[str, Any]\n",
    "    reasoning_steps: List[str]\n",
    "\n",
    "class GraphAnswerGenerator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the answer generator with Llama 3.3 via ChatGroq\"\"\"\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "    def format_graph_knowledge(self, retrieval_result) -> str:\n",
    "        \"\"\"\n",
    "        Convert graph retrieval results into structured text format\n",
    "\n",
    "        Args:\n",
    "            retrieval_result: Output from your ImprovedNeo4jGraphRetriever\n",
    "\n",
    "        Returns:\n",
    "            Formatted knowledge string\n",
    "        \"\"\"\n",
    "        knowledge_sections = []\n",
    "\n",
    "        # Section 1: Key Medical Entities Found\n",
    "        if retrieval_result.nodes:\n",
    "            entities_text = \"MEDICAL ENTITIES:\\n\"\n",
    "            # Group nodes by likely type (you could enhance this with node properties)\n",
    "            for i, node in enumerate(retrieval_result.nodes[:], 1):  # Limit for context\n",
    "                entities_text += f\"- {node}\\n\"\n",
    "            knowledge_sections.append(entities_text)\n",
    "\n",
    "        # Section 2: Medical Relationships\n",
    "        if retrieval_result.relationships:\n",
    "            relationships_text = \"MEDICAL RELATIONSHIPS:\\n\"\n",
    "            # Sort by importance if available\n",
    "            sorted_rels = sorted(\n",
    "                retrieval_result.relationships,\n",
    "                key=lambda x: x.get('importance', 0.5),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            for rel in sorted_rels[:20]:  # Top 20 relationships\n",
    "                source = rel.get('source', 'Unknown')\n",
    "                target = rel.get('target', 'Unknown')\n",
    "                rel_type = rel.get('relationship', 'RELATED_TO')\n",
    "\n",
    "                # Convert relationship types to natural language\n",
    "                rel_description = self._relationship_to_text(rel_type)\n",
    "                relationships_text += f\"- {source} {rel_description} {target}\\n\"\n",
    "\n",
    "            knowledge_sections.append(relationships_text)\n",
    "\n",
    "        # Section 3: Entity Matching Context\n",
    "        if retrieval_result.entity_matches:\n",
    "            matching_text = \"ENTITY CONTEXT:\\n\"\n",
    "            for match in retrieval_result.entity_matches:\n",
    "                if match.matched_nodes:\n",
    "                    matching_text += f\"- Query term '{match.entity}' relates to: {', '.join(match.matched_nodes[:3])}\\n\"\n",
    "            knowledge_sections.append(matching_text)\n",
    "\n",
    "        # Section 4: Connection Paths \n",
    "        if retrieval_result.path_info and retrieval_result.path_info.get('paths'):\n",
    "            paths_text = \"KEY CONNECTIONS:\\n\"\n",
    "            for path in retrieval_result.path_info['paths'][:5]:\n",
    "                start = path.get('start', '')\n",
    "                end = path.get('end', '')\n",
    "                relationships = path.get('relationships', [])\n",
    "                path_desc = ' → '.join(relationships) if relationships else 'connected to'\n",
    "                paths_text += f\"- {start} is {path_desc} {end}\\n\"\n",
    "            knowledge_sections.append(paths_text)\n",
    "\n",
    "        return \"\\n\".join(knowledge_sections)\n",
    "\n",
    "    def _relationship_to_text(self, rel_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Convertit un type de relation brut (Neo4j) en texte naturel :\n",
    "        - Met tout en minuscules\n",
    "        - Remplace les underscores '_' par des espaces\n",
    "        \"\"\"\n",
    "\n",
    "        return rel_type.lower().replace('_', ' ')\n",
    "\n",
    "    def create_answer_prompt(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Create the prompt template for answer generation\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a medical expert specializing in Type 2 Diabetes. Your role is to provide accurate, helpful answers based on the provided medical knowledge graph information.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY the provided medical knowledge\n",
    "2. Be precise , factual and explicative - avoid speculation\n",
    "3. Organize your answer logically (causes → mechanisms → treatments → outcomes)\n",
    "4. If the knowledge is insufficient, clearly state what information is missing\n",
    "5. Use medical terminology appropriately but explain complex concepts\n",
    "6. Focus on practical, clinically relevant information\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Direct answer first\n",
    "- Supporting details with mechanisms/relationships\n",
    "- Practical implications or recommendations\n",
    "- Note any limitations in the available knowledge\"\"\"),\n",
    "\n",
    "            (\"human\", \"\"\"Based on the following medical knowledge from a Type 2 Diabetes knowledge graph, please answer this question:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "AVAILABLE MEDICAL KNOWLEDGE:\n",
    "{formatted_knowledge}\n",
    "\n",
    "RETRIEVAL QUALITY: {confidence_score}/1.0\n",
    "REASONING CONTEXT: {retrieval_reasoning}\n",
    "\n",
    "Please provide a comprehensive answer based on this knowledge.\"\"\")\n",
    "        ])\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def generate_answer(self, question: str, retrieval_result) -> AnswerResult:\n",
    "        \"\"\"\n",
    "        Generate answer using structured knowledge from graph retrieval\n",
    "\n",
    "        Args:\n",
    "            question: Original user question\n",
    "            retrieval_result: Output GraphRetriever\n",
    "\n",
    "        Returns:\n",
    "            AnswerResult with generated answer and metadata\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Format the knowledge\n",
    "        formatted_knowledge = self.format_graph_knowledge(retrieval_result)\n",
    "\n",
    "        # Step 2: Create the prompt\n",
    "        prompt = self.create_answer_prompt()\n",
    "\n",
    "        # Step 3: Prepare prompt variables\n",
    "        prompt_vars = {\n",
    "            \"question\": question,\n",
    "            \"formatted_knowledge\": formatted_knowledge,\n",
    "            \"confidence_score\": retrieval_result.subgraph_score,\n",
    "            \"retrieval_reasoning\": retrieval_result.reasoning\n",
    "        }\n",
    "\n",
    "        # Step 4: Generate answer\n",
    "        try:\n",
    "            chain = prompt | self.llm\n",
    "            response = chain.invoke(prompt_vars)\n",
    "            answer_text = response.content\n",
    "\n",
    "            # Step 5: Estimate answer confidence based on retrieval quality\n",
    "            answer_confidence = self._calculate_answer_confidence(\n",
    "                retrieval_result, formatted_knowledge, answer_text\n",
    "            )\n",
    "\n",
    "            # Step 6: Extract reasoning steps (basic implementation)\n",
    "            reasoning_steps = self._extract_reasoning_steps(answer_text)\n",
    "\n",
    "            return AnswerResult(\n",
    "                answer=answer_text,\n",
    "                confidence=answer_confidence,\n",
    "                knowledge_used={\n",
    "                    \"entities_count\": len(retrieval_result.nodes),\n",
    "                    \"relationships_count\": len(retrieval_result.relationships),\n",
    "                    \"retrieval_score\": retrieval_result.subgraph_score,\n",
    "                    \"entity_matches\": len(retrieval_result.entity_matches)\n",
    "                },\n",
    "                reasoning_steps=reasoning_steps\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return AnswerResult(\n",
    "                answer=f\"Error generating answer: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                knowledge_used={},\n",
    "                reasoning_steps=[\"Error occurred during generation\"]\n",
    "            )\n",
    "\n",
    "    def _calculate_answer_confidence(self, retrieval_result, formatted_knowledge: str, answer: str) -> float:\n",
    "        \"\"\"Calculate confidence score for the generated answer\"\"\"\n",
    "\n",
    "        # Factor 1: Retrieval quality\n",
    "        retrieval_confidence = retrieval_result.subgraph_score\n",
    "\n",
    "        # Factor 2: Knowledge richness\n",
    "        knowledge_richness = min(len(formatted_knowledge) / 1000, 1.0)  # Normalize to 0-1\n",
    "\n",
    "        # Factor 3: Answer completeness (basic heuristic)\n",
    "        answer_completeness = min(len(answer) / 500, 1.0) if len(answer) > 50 else 0.3\n",
    "\n",
    "        # Factor 4: Entity coverage\n",
    "        entities_covered = len(retrieval_result.entity_matches) / max(len(retrieval_result.entity_matches), 1)\n",
    "\n",
    "        # Weighted average\n",
    "        total_confidence = (\n",
    "            0.4 * retrieval_confidence +\n",
    "            0.3 * knowledge_richness +\n",
    "            0.2 * answer_completeness +\n",
    "            0.1 * entities_covered\n",
    "        )\n",
    "\n",
    "        return min(total_confidence, 1.0)\n",
    "\n",
    "    def _extract_reasoning_steps(self, answer: str) -> List[str]:\n",
    "        \"\"\"Extract basic reasoning steps from the answer\"\"\"\n",
    "        # Simple implementation - you could make this more sophisticated\n",
    "        sentences = answer.split('. ')\n",
    "        reasoning_steps = []\n",
    "\n",
    "        for sentence in sentences[:5]:  # Take first 5 sentences as reasoning steps\n",
    "            if len(sentence.strip()) > 20:\n",
    "                reasoning_steps.append(sentence.strip())\n",
    "\n",
    "        return reasoning_steps\n",
    "\n",
    "# ==================== COMPLETE PIPELINE TEST ====================\n",
    "\n",
    "def test_complete_pipeline(neo4j_graph):\n",
    "    \"\"\"Test the complete end-to-end pipeline\"\"\"\n",
    "\n",
    "    # Initialize components\n",
    "    retriever = Neo4jGraphRetriever(neo4j_graph)  # Your existing retriever\n",
    "    answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "    # Test question\n",
    "    test_question = \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\"\n",
    "\n",
    "    print(f\"QUESTION: {test_question}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Step 1: Extract entities (your existing code)\n",
    "    entity_result = entity_chain.invoke({\"question\": test_question})\n",
    "    entities = entity_result.names\n",
    "    print(f\"ENTITIES: {entities}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Retrieve relevant subgraph info\n",
    "    retrieval_result = retriever.retrieve_subgraph_info(entities, k_hops=2)\n",
    "    # Step 2: Retrieve knowledge graph\n",
    "    #retrieval_result = retriever.retrieve_subgraph_advanced(extracted_entities, question)\n",
    "\n",
    "    # Step 3: Generate answer\n",
    "    answer_result = answer_generator.generate_answer(test_question, retrieval_result)\n",
    "\n",
    "    print(f\"\\n GENERATED ANSWER:\")\n",
    "    print(\"-\"*50)\n",
    "    print(answer_result.answer)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    return answer_result\n",
    "\n",
    "\n",
    "def test_complet_pipeline(neo4j_graph):\n",
    "    \"\"\"Test the complete end-to-end pipeline\"\"\"\n",
    "\n",
    "    # Initialize components\n",
    "    retriever = GraphRetriever(neo4j_graph)\n",
    "    answer_generator = GraphAnswerGenerator()\n",
    "\n",
    "    # Test question\n",
    "    test_question = \"what are the symptoms of type 2 diabetes?\"\n",
    "\n",
    "    print(f\"🔍 QUESTION: {test_question}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Step 1: Extract entities (replace with your actual entity extraction chain)\n",
    "    # For demo, hardcoding:\n",
    "    entities = entity_chain.invoke({\"question\": question}).names\n",
    "\n",
    "    print(f\" ENTITIES: {entities}\")\n",
    "\n",
    "    # Step 2: Retrieve knowledge graph\n",
    "    retrieval_result = retriever.retrieve_subgraph_advanced(extracted_entities, question,k_hops=2)\n",
    "    # Step 3: Generate answer\n",
    "    answer_result = answer_generator.generate_answer(test_question, retrieval_result)\n",
    "\n",
    "    print(f\"\\n GENERATED ANSWER:\")\n",
    "    print(\"-\"*50)\n",
    "    print(answer_result.answer)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    return answer_result\n",
    "\n",
    "\n",
    "result = test_complete_pipeline(graph)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c2350-2191-46d7-b672-f7e32213447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9982dbb-c944-46fc-9f99-7abefcff2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "vector_store = Neo4jVector(\n",
    "    embedding=embedding_model,\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    node_label=\"Document\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,  \n",
    "        \"score_threshold\": 0.7 \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243023c-0e0f-4a11-ac8a-a4ab16847dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# --- Prompt personnalisé ---\n",
    "prompt_template = \"\"\"You are a medical assistant specialized in type 2 diabetes.\n",
    "Use the retrieved context to answer the question factually.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=\"api_key\",   \n",
    "    model=\"llama-3.3-70b-versatile\",       \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# --- QA chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "\n",
    "# --- Test ---\n",
    "query = \"What are the first warning signs and symptoms of type 2 diabetes?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee0b6a-0db0-4942-87be-157301f9b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"What are the risk factors for type 2 diabetes?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20f0cc-69c6-4184-a1b3-4bee1cd50501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"5.\tWhich organs are most affected by insulin resistance in T2D?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7732fa-449e-4c19-a81b-7ac184656ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f7e4e-ffbb-41a8-89b1-4b658e40c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"What are the mechanisms by which GLP-1 receptor agonists improve cardiovascular outcomes in patients with type 2 diabetes?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d622b9-1fd6-4030-b764-c0b0d1820e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"Why is lifestyle modification (diet and exercise) often recommended before or alongside medication for managing type 2 diabetes?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98656d-ca31-4104-896b-49a3d46fc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"What diet is recommended for someone with type 2 diabetes?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70170-c5f6-474d-8d84-425e37378e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test ---\n",
    "query = \"How should blood glucose be monitored?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1829a7-2248-4e1a-b1a5-e5f59990cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6d204-127d-48a7-939e-f3d0793bb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "\n",
    "class AdvancedRAGRetriever:\n",
    "    def __init__(self, vector_store, llm, embedding_model):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embedding_model\n",
    "        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def create_vector_retriever(self, k=10):\n",
    "        \"\"\"\n",
    "        Crée un retriever vectoriel optimisé\n",
    "        \"\"\"\n",
    "        vector_retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"mmr\",  # Maximum Marginal Relevance pour diversité\n",
    "            search_kwargs={\n",
    "                \"k\": k * 2,  # Récupère plus de docs pour le re-ranking\n",
    "                \"lambda_mult\": 0.7,  # Balance pertinence vs diversité\n",
    "                \"fetch_k\": k * 4,    # Docs candidats avant MMR\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return vector_retriever\n",
    "\n",
    "    def create_compression_retriever(self, base_retriever, k=10):\n",
    "        \"\"\"\n",
    "        Ajoute de la compression contextuelle pour améliorer la qualité\n",
    "        \"\"\"\n",
    "        # Filtre par similarité d'embeddings\n",
    "        embeddings_filter = EmbeddingsFilter(\n",
    "            embeddings=self.embedding_model,\n",
    "            similarity_threshold=0.76,  # Seuil de similarité\n",
    "            k=k * 2\n",
    "        )\n",
    "\n",
    "        # Supprime les doublons\n",
    "        redundant_filter = EmbeddingsRedundantFilter(\n",
    "            embeddings=self.embedding_model,\n",
    "            similarity_threshold=0.95\n",
    "        )\n",
    "\n",
    "        # Extracteur LLM pour les passages les plus pertinents\n",
    "        llm_extractor = LLMChainExtractor.from_llm(self.llm)\n",
    "\n",
    "        # Pipeline de compression\n",
    "        pipeline_compressor = DocumentCompressorPipeline(\n",
    "            transformers=[embeddings_filter, redundant_filter, llm_extractor]\n",
    "        )\n",
    "\n",
    "        # Retriever avec compression\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline_compressor,\n",
    "            base_retriever=base_retriever\n",
    "        )\n",
    "\n",
    "        return compression_retriever\n",
    "\n",
    "    def create_multi_query_retriever(self, base_retriever):\n",
    "        \"\"\"\n",
    "        Génère plusieurs variantes de la question pour une meilleure recherche\n",
    "        \"\"\"\n",
    "        multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "            retriever=base_retriever,\n",
    "            llm=self.llm,\n",
    "            prompt=self._get_multi_query_prompt()\n",
    "        )\n",
    "        return multi_query_retriever\n",
    "\n",
    "    def rerank_with_cross_encoder(self, query, documents, top_k=5):\n",
    "        \"\"\"\n",
    "        Re-rank les documents avec un cross-encoder pour améliorer la pertinence\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return documents\n",
    "\n",
    "        # Prépare les paires query-document\n",
    "        pairs = [(query, doc.page_content) for doc in documents]\n",
    "\n",
    "        # Calcule les scores de pertinence\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "\n",
    "        # Combine documents et scores\n",
    "        doc_scores = list(zip(documents, scores))\n",
    "\n",
    "        # Trie par score décroissant\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Retourne les top_k documents\n",
    "        return [doc for doc, score in doc_scores[:top_k]]\n",
    "\n",
    "    def create_complete_advanced_retriever(self, k=10):\n",
    "        \"\"\"\n",
    "        Crée le retriever le plus avancé combinant toutes les techniques\n",
    "        \"\"\"\n",
    "        # 1. Retriever vectoriel de base\n",
    "        vector_retriever = self.create_vector_retriever(k=k*2)\n",
    "\n",
    "        # 2. Multi-query pour diversifier les recherches\n",
    "        multi_query_retriever = self.create_multi_query_retriever(vector_retriever)\n",
    "\n",
    "        # 3. Compression contextuelle\n",
    "        final_retriever = self.create_compression_retriever(multi_query_retriever, k=k)\n",
    "\n",
    "        return final_retriever\n",
    "\n",
    "    def advanced_similarity_search(self, query, k=5, include_reranking=True):\n",
    "        \"\"\"\n",
    "        Recherche avancée avec re-ranking optionnel\n",
    "        \"\"\"\n",
    "        # Recherche initiale avec plus de documents\n",
    "        initial_docs = self.vector_store.similarity_search_with_score(\n",
    "            query, k=k*3, score_threshold=0.7\n",
    "        )\n",
    "\n",
    "        documents = [doc for doc, score in initial_docs]\n",
    "\n",
    "        if include_reranking and documents:\n",
    "            # Re-ranking avec cross-encoder\n",
    "            documents = self.rerank_with_cross_encoder(query, documents, k)\n",
    "        else:\n",
    "            documents = documents[:k]\n",
    "\n",
    "        # Réorganise pour un meilleur contexte\n",
    "        reordered_docs = LongContextReorder().transform_documents(documents)\n",
    "\n",
    "        return reordered_docs\n",
    "\n",
    "    def evaluate_retrieval_quality(self, query, retrieved_docs, ground_truth=None):\n",
    "        \"\"\"\n",
    "        Évalue la qualité du retrieval\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Vérification si on a des documents\n",
    "        if not retrieved_docs:\n",
    "            return {\"error\": \"No documents retrieved\"}\n",
    "\n",
    "        # Diversité des documents\n",
    "        if len(retrieved_docs) > 1:\n",
    "            embeddings = [self.embedding_model.embed_query(doc.page_content)\n",
    "                         for doc in retrieved_docs]\n",
    "            similarities = []\n",
    "            for i in range(len(embeddings)):\n",
    "                for j in range(i+1, len(embeddings)):\n",
    "                    sim = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                        np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])\n",
    "                    )\n",
    "                    similarities.append(sim)\n",
    "\n",
    "            metrics['diversity_score'] = 1 - np.mean(similarities) if similarities else 0\n",
    "\n",
    "        # Pertinence moyenne avec cross-encoder\n",
    "        query_doc_pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
    "        relevance_scores = self.cross_encoder.predict(query_doc_pairs)\n",
    "        metrics['avg_relevance'] = np.mean(relevance_scores)\n",
    "        metrics['min_relevance'] = np.min(relevance_scores)\n",
    "        metrics['max_relevance'] = np.max(relevance_scores)\n",
    "\n",
    "        # Longueur des documents\n",
    "        doc_lengths = [len(doc.page_content) for doc in retrieved_docs]\n",
    "        metrics['avg_doc_length'] = np.mean(doc_lengths)\n",
    "        metrics['total_content_length'] = sum(doc_lengths)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _get_multi_query_prompt(self):\n",
    "        \"\"\"Prompt optimisé pour la génération de queries multiples\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an expert medical assistant. Given a medical question about diabetes,\n",
    "            generate 3 different versions of this question that could help retrieve relevant information.\n",
    "            Make the questions more specific and include relevant medical terminology when appropriate.\n",
    "\n",
    "            Original question: {question}\n",
    "\n",
    "            Alternative questions:\n",
    "            1.\n",
    "            2.\n",
    "            3.\n",
    "            \"\"\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "# === UTILISATION AVANCÉE ===\n",
    "def setup_advanced_rag_system(vector_store, llm, embedding_model):\n",
    "    \"\"\"\n",
    "    Configure un système RAG avancé - version sans BM25/TFIDF\n",
    "    \"\"\"\n",
    "    # Initialise le retriever avancé\n",
    "    advanced_retriever = AdvancedRAGRetriever(vector_store, llm, embedding_model)\n",
    "\n",
    "    # Crée le retriever complet (sans avoir besoin de documents)\n",
    "    try:\n",
    "        complete_retriever = advanced_retriever.create_complete_advanced_retriever(k=5)\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la création du retriever complet: {e}\")\n",
    "        print(\"Utilisation du retriever de base...\")\n",
    "        complete_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Template de prompt \n",
    "    improved_template = \"\"\"\n",
    "    You are an expert medical assistant specialized in type 2 diabetes.\n",
    "    Use ONLY and STRICTLY the provided medical context to answer questions accurately and comprehensively.\n",
    "\n",
    "    GUIDELINES:\n",
    "    - Prioritize information from the most relevant sources\n",
    "    - Provide detailed explanations with medical reasoning\n",
    "    - If information is incomplete, clearly state what's missing\n",
    "    - Include relevant context and background information\n",
    "    - Use professional medical language but keep it accessible\n",
    "    - If an information is not stated in the provided context stated it clearly\n",
    "\n",
    "    RELEVANT MEDICAL CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    PATIENT QUESTION: {question}\n",
    "\n",
    "    COMPREHENSIVE MEDICAL ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    improved_prompt = PromptTemplate(\n",
    "        template=improved_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Crée la chaîne QA \n",
    "    advanced_qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=complete_retriever,\n",
    "        chain_type_kwargs={\"prompt\": improved_prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return advanced_qa_chain, advanced_retriever\n",
    "\n",
    "\n",
    "--- Test Fuunction --\n",
    "\n",
    "def ask_advanced_question(advanced_qa_chain, advanced_retriever, question):\n",
    "    \"\"\"\n",
    "    Pose une question au système RAG avancé\n",
    "    \"\"\"\n",
    "    print(f\"\\n Question: {question}\")\n",
    "\n",
    "    try:\n",
    "        result = advanced_qa_chain({\"query\": question})\n",
    "        print(f\" Réponse améliorée: {result['result']}\")\n",
    "\n",
    "        # Évalue la qualité si possible\n",
    "        if advanced_retriever and result.get('source_documents'):\n",
    "            metrics = advanced_retriever.evaluate_retrieval_quality(\n",
    "                question, result['source_documents']\n",
    "            )\n",
    "            print(f\" Score de pertinence: {metrics.get('avg_relevance', 0):.3f}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la réponse: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# === EXEMPLE D'UTILISATION ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration de base \n",
    "    GROQ_API_KEY = \"api_key\"\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama3-70b-8192\",\n",
    "        temperature=0,\n",
    "        top_p=0.9,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "    vector_store = vector_store\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "      model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "      encode_kwargs={\"normalize_embeddings\": True}\n",
    "      )\n",
    "\n",
    "\n",
    "    advanced_qa_chain, advanced_retriever = setup_advanced_rag_system(\n",
    "        vector_store, llm, embedding_model\n",
    "    )\n",
    "\n",
    "    # Tests\n",
    "    if advanced_qa_chain and advanced_retriever:\n",
    "\n",
    "         ask_advanced_question(advanced_qa_chain, advanced_retriever,\n",
    "                              \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7665c79-598f-4937-a33c-ea2e9916dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXEMPLE D'UTILISATION ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration de base\n",
    "    GROQ_API_KEY = \"API_Key\"\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0,\n",
    "        top_p=0.9,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "    vector_store = vector_store\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "      model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "      encode_kwargs={\"normalize_embeddings\": True}\n",
    "      )\n",
    "\n",
    "    advanced_qa_chain, advanced_retriever = setup_advanced_rag_system(\n",
    "        vector_store, llm, embedding_model\n",
    "    )\n",
    "\n",
    "    # Tests\n",
    "    if advanced_qa_chain and advanced_retriever:\n",
    "\n",
    "         ask_advanced_question(advanced_qa_chain, advanced_retriever,\n",
    "                              \"Which organs are most affected by insulin resistance in T2D?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec03418-75e0-496e-8c27-0ae5d178ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643111ec-f769-4e5b-9a05-5db993d238a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, vector_retriever: AdvancedRAGRetriever, graph_retriever: GraphRetriever,\n",
    "                 alpha: float = 0.7):\n",
    "        \"\"\"\n",
    "        Hybrid Retriever combinant vector-based et graph-based retrieval\n",
    "\n",
    "        Args:\n",
    "            vector_retriever: Instance de AdvancedRAGRetriever\n",
    "            graph_retriever: Instance de GraphRetriever\n",
    "            alpha: Poids du vector retriever (0.0 à 1.0, plus haut = plus d'importance au vectoriel)\n",
    "        \"\"\"\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.graph_retriever = graph_retriever\n",
    "        self.alpha = alpha  \n",
    "\n",
    "    def retrieve(self, question: str, k: int = 5, min_graph_relevance: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Exécute un retrieval hybride et combine les résultats des deux approches.\n",
    "\n",
    "        Returns:\n",
    "            dict contenant:\n",
    "              - hybrid_documents: liste finale pondérée\n",
    "              - vector_docs: docs du retriever vectoriel\n",
    "              - graph_result: sous-graphe du retriever graphe\n",
    "              - scores: détails des scores de fusion\n",
    "        \"\"\"\n",
    "        # === 1. Retrieval vectoriel  ===\n",
    "        vector_docs = self.vector_retriever.advanced_similarity_search(question, k=k)\n",
    "        vector_scores = self.vector_retriever.evaluate_retrieval_quality(question, vector_docs)\n",
    "        avg_vector_score = vector_scores.get(\"avg_relevance\", 0.5)\n",
    "\n",
    "        # === 2. Retrieval graphe ===\n",
    "        graph_result = self.graph_retriever.retrieve_for_question(question, min_relevance=min_graph_relevance)\n",
    "        graph_score = graph_result.subgraph_score if graph_result else 0.0\n",
    "\n",
    "        # === 3. Fusion pondérée des scores ===\n",
    "        final_score = self.alpha * avg_vector_score + (1 - self.alpha) * graph_score\n",
    "\n",
    "        # === 4. Résultat final ===\n",
    "        return {\n",
    "            \"hybrid_score\": final_score,\n",
    "            \"vector_score\": avg_vector_score,\n",
    "            \"graph_score\": graph_score,\n",
    "            \"vector_docs\": vector_docs,\n",
    "            \"graph_result\": graph_result,\n",
    "            \"reasoning\": f\"Fusion avec alpha={self.alpha:.2f}: \"\n",
    "                         f\"vector={avg_vector_score:.2f}, graph={graph_score:.2f}, \"\n",
    "                         f\"final={final_score:.2f}\"\n",
    "        }\n",
    "\n",
    "\n",
    "# === EXEMPLE D'UTILISATION ===\n",
    "'''if __name__ == \"__main__\":\n",
    "    advanced_retriever = AdvancedRAGRetriever(\n",
    "        vector_store=vector_store,\n",
    "        llm=llm,\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "    graph_retriever = GraphRetriever(graph)\n",
    "\n",
    "    #hybrid = HybridRetriever(advanced_retriever, graph_retriever, alpha=0.7)\n",
    "\n",
    "    #question = \"What are the most effective medications for lowering HbA1c in Type 2 diabetes?\"\n",
    "    #result = hybrid.retrieve(question, k=5)\n",
    "\n",
    "    print(\"\\n=== RESULTAT HYBRIDE ===\")\n",
    "    print(f\"Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "    print(f\"Vector Score: {result['vector_score']:.3f}\")\n",
    "    print(f\"Graph Score: {result['graph_score']:.3f}\")\n",
    "    print(\"Reasoning:\", result[\"reasoning\"])\n",
    "\n",
    "    # Afficher un résumé des documents vectoriels\n",
    "    for i, doc in enumerate(result[\"vector_docs\"], 1):\n",
    "        print(f\"\\nDoc {i}:\", doc.page_content[:200], \"...\")\n",
    "\n",
    "    # Afficher un résumé du sous-graphe\n",
    "    print(\"\\nGraph Subgraph Score:\", result[\"graph_result\"].subgraph_score)\n",
    "    print(\"Graph Nodes:\", result[\"graph_result\"].nodes[:10])\n",
    "    print(\"Graph Reasoning:\", result[\"graph_result\"].reasoning)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af4066-75c5-4fe0-84c9-7fa827cb267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retriever = AdvancedRAGRetriever(\n",
    "        vector_store=vector_store,\n",
    "        llm=llm,\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "graph_retriever = GraphRetriever(graph)\n",
    "\n",
    "hybrid = HybridRetriever(advanced_retriever, graph_retriever, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b7bb1-3bff-4064-b616-3132878c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAnswerGenerator:\n",
    "    def __init__(self, hybrid_retriever, llm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hybrid_retriever: Instance of HybridRetriever\n",
    "            llm: LLM callable (ex: OpenAI, Groq, HuggingFace pipeline)\n",
    "        \"\"\"\n",
    "        self.hybrid_retriever = hybrid_retriever\n",
    "        self.llm = llm\n",
    "\n",
    "    def __call__(self, question: str, k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Generates a final natural answer as if directly from a powerful LLM.\n",
    "        \"\"\"\n",
    "        # 1. Retrieve best context (vector + graph)\n",
    "        retrieval_result = self.hybrid_retriever.retrieve(question, k=k)\n",
    "\n",
    "        docs_text = \"\\n\".join([doc.page_content for doc in retrieval_result[\"vector_docs\"]])\n",
    "        graph_summary = retrieval_result[\"graph_result\"].reasoning if retrieval_result[\"graph_result\"] else \"\"\n",
    "\n",
    "        # 2. Build prompt for LLM\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert medical assistant specializing in Type 2 Diabetes (T2D). Your task is to answer the patient's question accurately, using both textual medical context and graph-based knowledge.\n",
    "\n",
    "GUIDELINES:\n",
    "1. Prioritize information from the most reliable and relevant sources.\n",
    "2. Provide clear, detailed explanations with medical reasoning.\n",
    "3. Mention if information is missing or uncertain.\n",
    "4. Include important background or context for the patient to understand.\n",
    "5. Use professional yet accessible language for healthcare communication.\n",
    "6. When applicable, reference medications, lifestyle interventions, or clinical guidelines.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "TEXTUAL MEDICAL CONTEXT:\n",
    "{docs_text}\n",
    "\n",
    "GRAPH-BASED KNOWLEDGE SUMMARY:\n",
    "{graph_summary}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Combine insights from textual context and graph knowledge.\n",
    "- Focus on actionable, evidence-based recommendations.\n",
    "- Avoid speculation; clearly state any uncertainties.\n",
    "\n",
    "FINAL ANSWER:\n",
    "        \"\"\"\n",
    "\n",
    "        # 3. Generate & return final answer\n",
    "        return self.llm(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daf1f1-ccfc-42ef-a70b-3bc5373f4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"API_Key\")\n",
    "\n",
    "def llama3_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Appelle LLaMA 3 via ChatGroq pour générer une réponse.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return response.choices[0].message.content \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bd31f-1b22-48e8-a9d7-fb33f1db629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What are the first warning signs and symptoms of type 2 diabetes?\"\n",
    "answer = answer_generator(question)\n",
    "\n",
    "print(\"\\n=== FINAL ANSWER ===\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a84c53-f742-4f17-bea0-5db680111be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What are three ways to prevent type 2 diabetes? \"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728edd82-94aa-49cc-ac2b-b2210f7e6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What are three ways to prevent type 2 diabetes? \"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233d332-e9e8-4bb1-9d00-a249bfa33d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What is the first-line medication recommended by the ADA for most people with newly diagnosed type 2 diabetes? \"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de28fe-d1a8-4f72-875b-987201ba4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What are the risk factors for type 2 diabetes?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddff88-e1d7-4ce8-943d-d1f40e852024",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"Which organs are most affected by insulin resistance in T2D?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb924c24-fba7-4449-b288-7d530e25cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What are the most effective medications for controlling HbA1c levels in newly diagnosed Type 2 diabetes patients?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434276ac-59b2-4ccc-83b8-d57c55053adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"What diet is recommended for someone with type 2 diabetes?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2174e4b-0920-4b41-a906-66336b82a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"How should blood glucose be monitored?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29b9f2-64c2-401c-a3d5-7f8edbf2d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator = HybridAnswerGenerator(hybrid, llama3_llm)\n",
    "\n",
    "question = \"Can type 2 diabetes be prevented?\"\n",
    "answer = answer_generator(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77fa9c-73ba-429c-8759-1247030c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832a849-5eac-45e1-800f-fe75b96fd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "class HybridAnswerGenerator:\n",
    "    def __init__(self, hybrid_retriever, llm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hybrid_retriever: Instance of HybridRetriever\n",
    "            llm: LLM callable \n",
    "        \"\"\"\n",
    "        self.hybrid_retriever = hybrid_retriever\n",
    "        self.llm = llm\n",
    "\n",
    "    def __call__(self, question: str, k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Generates a final natural answer as if directly from a powerful LLM.\n",
    "        \"\"\"\n",
    "        # 1. Retrieve best context (vector + graph)\n",
    "        retrieval_result = self.hybrid_retriever.retrieve(question, k=k)\n",
    "        print(\"DEBUG:\", type(retrieval_result), retrieval_result)\n",
    "\n",
    "        docs = retrieval_result[\"vector_docs\"]\n",
    "\n",
    "        def extract_content(doc):\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                return doc.page_content\n",
    "            elif hasattr(doc, 'content'):\n",
    "                return doc.content\n",
    "            else:\n",
    "                return str(doc)\n",
    "\n",
    "        docs_text = \"\\n\".join([extract_content(doc) for doc in docs])\n",
    "        graph_summary = retrieval_result[\"graph_result\"].reasoning if retrieval_result[\"graph_result\"] else \"\"\n",
    "\n",
    "        # 2. Build prompt for LLM\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert medical assistant specializing in Type 2 Diabetes (T2D). Your task is to answer the patient's question accurately, using both textual medical context and graph-based knowledge.\n",
    "\n",
    "GUIDELINES:\n",
    "1. Prioritize information from the most reliable and relevant sources.\n",
    "2. Provide clear, detailed explanations with medical reasoning.\n",
    "3. Mention if information is missing or uncertain.\n",
    "4. Include important background or context for the patient to understand.\n",
    "5. Use professional yet accessible language for healthcare communication.\n",
    "6. When applicable, reference medications, lifestyle interventions, or clinical guidelines.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "TEXTUAL MEDICAL CONTEXT:\n",
    "{docs_text}\n",
    "\n",
    "GRAPH-BASED KNOWLEDGE SUMMARY:\n",
    "{graph_summary}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Combine insights from textual context and graph knowledge.\n",
    "- Focus on actionable, evidence-based recommendations.\n",
    "- Avoid speculation; clearly state any uncertainties.\n",
    "\n",
    "FINAL ANSWER:\n",
    "        \"\"\"\n",
    "\n",
    "        # 3. Generate & return final answer\n",
    "        return self.llm(prompt)\n",
    "\n",
    "def create_minimal_interface(hybrid_generator: HybridAnswerGenerator):\n",
    "    \"\"\"\n",
    "    Creates a minimalist medical interface for HybridAnswerGenerator\n",
    "    \"\"\"\n",
    "\n",
    "    def process_question(question: str, k_value: int, progress=gr.Progress()):\n",
    "        \"\"\"\n",
    "        Process medical question with progress tracking\n",
    "        \"\"\"\n",
    "        if not question.strip():\n",
    "            return \"Please enter a question before submitting.\", None, \"⚠ Empty question\"\n",
    "\n",
    "        progress(0.3, desc=\"Analyzing question...\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        progress(0.7, desc=\"Retrieving knowledge...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        progress(0.9, desc=\"Generating response...\")\n",
    "\n",
    "        try:\n",
    "            answer = hybrid_generator(question, k=k_value)\n",
    "            progress(1.0, desc=\"Complete\")\n",
    "\n",
    "            debug_info = f\"**Query parameters:** k={k_value} | Length: {len(question)} chars\"\n",
    "\n",
    "            return answer, debug_info, \"✓ Response generated\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\", None, \"✗ Generation failed\"\n",
    "\n",
    "    # Minimal CSS with only 3 colors: white (#ffffff), dark blue (#2c3e50), light gray (#f8f9fa)\n",
    "    minimal_css = \"\"\"\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600&display=swap');\n",
    "\n",
    "    * {\n",
    "        font-family: 'Fira Code', monospace !important;\n",
    "    }\n",
    "\n",
    "    .gradio-container {\n",
    "        background: #ffffff !important;\n",
    "        color: #2c3e50 !important;\n",
    "    }\n",
    "\n",
    "    .main-container {\n",
    "        max-width: 1000px;\n",
    "        margin: 0 auto;\n",
    "        padding: 40px 20px;\n",
    "    }\n",
    "\n",
    "    h1, h2, h3 {\n",
    "        color: #2c3e50 !important;\n",
    "        font-weight: 500 !important;\n",
    "        margin: 0 0 20px 0 !important;\n",
    "    }\n",
    "\n",
    "    .header {\n",
    "        text-align: center;\n",
    "        margin-bottom: 40px;\n",
    "        padding-bottom: 20px;\n",
    "        border-bottom: 1px solid #f8f9fa;\n",
    "    }\n",
    "\n",
    "    .warning-box {\n",
    "        background: #f8f9fa !important;\n",
    "        border: 1px solid #2c3e50 !important;\n",
    "        border-radius: 4px !important;\n",
    "        padding: 15px !important;\n",
    "        margin: 20px 0 !important;\n",
    "        color: #2c3e50 !important;\n",
    "        font-size: 0.9em !important;\n",
    "    }\n",
    "\n",
    "    .input-group, .output-group {\n",
    "        background: #ffffff !important;\n",
    "        border: 1px solid #f8f9fa !important;\n",
    "        border-radius: 4px !important;\n",
    "        padding: 20px !important;\n",
    "        margin: 15px 0 !important;\n",
    "    }\n",
    "\n",
    "    button {\n",
    "        background: #2c3e50 !important;\n",
    "        color: #ffffff !important;\n",
    "        border: 1px solid #2c3e50 !important;\n",
    "        border-radius: 4px !important;\n",
    "        padding: 10px 20px !important;\n",
    "        font-weight: 400 !important;\n",
    "        transition: all 0.2s ease !important;\n",
    "    }\n",
    "\n",
    "    button:hover {\n",
    "        background: #ffffff !important;\n",
    "        color: #2c3e50 !important;\n",
    "        border: 1px solid #2c3e50 !important;\n",
    "    }\n",
    "\n",
    "    .secondary-button {\n",
    "        background: #ffffff !important;\n",
    "        color: #2c3e50 !important;\n",
    "        border: 1px solid #f8f9fa !important;\n",
    "    }\n",
    "\n",
    "    .secondary-button:hover {\n",
    "        background: #f8f9fa !important;\n",
    "        color: #2c3e50 !important;\n",
    "    }\n",
    "\n",
    "    input, textarea {\n",
    "        border: 1px solid #f8f9fa !important;\n",
    "        background: #ffffff !important;\n",
    "        color: #2c3e50 !important;\n",
    "        border-radius: 4px !important;\n",
    "    }\n",
    "\n",
    "    input:focus, textarea:focus {\n",
    "        border-color: #2c3e50 !important;\n",
    "        outline: none !important;\n",
    "    }\n",
    "\n",
    "    .examples-grid {\n",
    "        display: grid;\n",
    "        grid-template-columns: 1fr;\n",
    "        gap: 8px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "\n",
    "    .example-btn {\n",
    "        background: #f8f9fa !important;\n",
    "        color: #2c3e50 !important;\n",
    "        border: 1px solid #f8f9fa !important;\n",
    "        text-align: left !important;\n",
    "        padding: 12px 16px !important;\n",
    "        font-size: 0.9em !important;\n",
    "    }\n",
    "\n",
    "    .example-btn:hover {\n",
    "        border-color: #2c3e50 !important;\n",
    "    }\n",
    "\n",
    "    .status-text {\n",
    "        font-size: 0.9em;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    with gr.Blocks(css=minimal_css, title=\"T2D Medical Assistant\", theme=gr.themes.Default()) as interface:\n",
    "\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "\n",
    "            # Simple header\n",
    "            with gr.Row(elem_classes=[\"header\"]):\n",
    "                gr.HTML(\"\"\"\n",
    "                    <h1>T2D Medical Assistant</h1>\n",
    "                    <p>Hybrid AI system combining vector search and graph knowledge</p>\n",
    "                \"\"\")\n",
    "\n",
    "            # Medical disclaimer\n",
    "            gr.HTML(\"\"\"\n",
    "                <div class=\"warning-box\">\n",
    "                    <strong>Medical Disclaimer:</strong> This AI assistant provides general information about Type 2 Diabetes for educational purposes only.\n",
    "                    It does not replace professional medical advice. Always consult your healthcare provider for personalized medical guidance.\n",
    "                </div>\n",
    "            \"\"\")\n",
    "\n",
    "            # Input section\n",
    "            with gr.Group(elem_classes=[\"input-group\"]):\n",
    "                gr.Markdown(\"### Ask your question\")\n",
    "\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"Medical question about Type 2 Diabetes\",\n",
    "                    placeholder=\"Example: What are the side effects of metformin? How does exercise affect blood glucose?\",\n",
    "                    lines=3,\n",
    "                    max_lines=6\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    k_slider = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=10,\n",
    "                        value=5,\n",
    "                        step=1,\n",
    "                        label=\"Number of sources (k)\",\n",
    "                        info=\"More sources = comprehensive but slower response\"\n",
    "                    )\n",
    "\n",
    "                    submit_btn = gr.Button(\n",
    "                        \"Generate Response\",\n",
    "                        variant=\"primary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "\n",
    "            # Output section\n",
    "            with gr.Group(elem_classes=[\"output-group\"]):\n",
    "                gr.Markdown(\"### Medical Response\")\n",
    "\n",
    "                status_output = gr.Textbox(\n",
    "                    label=\"Status\",\n",
    "                    interactive=False,\n",
    "                    show_label=False,\n",
    "                    elem_classes=[\"status-text\"]\n",
    "                )\n",
    "\n",
    "                answer_output = gr.Textbox(\n",
    "                    label=\"AI Response\",\n",
    "                    lines=12,\n",
    "                    max_lines=20,\n",
    "                    show_copy_button=True,\n",
    "                    interactive=False,\n",
    "                    placeholder=\"The AI response will appear here...\"\n",
    "                )\n",
    "\n",
    "                with gr.Accordion(\"Technical Details\", open=False):\n",
    "                    debug_output = gr.Markdown(\"Processing details will appear after response generation.\")\n",
    "\n",
    "            # Example questions\n",
    "            with gr.Group():\n",
    "                gr.Markdown(\"### Example Questions\")\n",
    "\n",
    "                examples = [\n",
    "                    \"What are the side effects of metformin and how to manage them?\",\n",
    "                    \"How does physical exercise affect blood glucose in T2D patients?\",\n",
    "                    \"Which foods should be avoided with Type 2 Diabetes?\",\n",
    "                    \"How to recognize hypoglycemia symptoms and what to do?\",\n",
    "                    \"What is the difference between Type 1 and Type 2 diabetes?\"\n",
    "                ]\n",
    "\n",
    "                with gr.Column(elem_classes=[\"examples-grid\"]):\n",
    "                    for example in examples:\n",
    "                        gr.Button(\n",
    "                            example,\n",
    "                            elem_classes=[\"example-btn\", \"secondary-button\"],\n",
    "                            size=\"sm\"\n",
    "                        ).click(\n",
    "                            fn=lambda x=example: x,\n",
    "                            outputs=question_input\n",
    "                        )\n",
    "\n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            fn=process_question,\n",
    "            inputs=[question_input, k_slider],\n",
    "            outputs=[answer_output, debug_output, status_output],\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        question_input.submit(\n",
    "            fn=process_question,\n",
    "            inputs=[question_input, k_slider],\n",
    "            outputs=[answer_output, debug_output, status_output],\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        # Footer\n",
    "        gr.HTML(\"\"\"\n",
    "            <div style=\"text-align: center; margin-top: 40px; padding: 20px 0;\n",
    "                        border-top: 1px solid #f8f9fa; color: #2c3e50;\">\n",
    "                <p>Hybrid AI System | Vector Search + Graph Knowledge | Medical Assistant for T2D</p>\n",
    "            </div>\n",
    "        \"\"\")\n",
    "\n",
    "    return interface\n",
    "\n",
    "# Usage example\n",
    "def demo_usage():\n",
    "    \"\"\"\n",
    "    Example usage of the interface\n",
    "    \"\"\"\n",
    "    hybrid_retriever = HybridRetriever(advanced_retriever, graph_retriever, alpha=0.7)\n",
    "    llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",   \n",
    "    temperature=0,\n",
    "    )\n",
    "\n",
    "    hybrid_generator = HybridAnswerGenerator(hybrid_retriever, llm)\n",
    "    interface = create_minimal_interface(hybrid_generator)\n",
    "    # Launch the interface\n",
    "    interface.launch(\n",
    "         server_name=\"0.0.0.0\",\n",
    "         server_port=7866,\n",
    "         share=False,\n",
    "         inbrowser=True\n",
    "     )\n",
    "\n",
    "    print(\"Interface ready to launch!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
