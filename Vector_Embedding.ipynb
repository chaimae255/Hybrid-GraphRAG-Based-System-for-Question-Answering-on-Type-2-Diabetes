{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51ef867-019e-4aa0-a966-b3dcebe75371",
   "metadata": {},
   "source": [
    " <h4> Biomedical Document Ingestion & Vector Indexing into Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b10c06-f272-44d9-8c9c-090521fc4a94",
   "metadata": {},
   "source": [
    "This script processes a collection of biomedical PDFs (2020 - 2025) related to Type 2 Diabetes, splits them into semantically meaningful chunks, and stores them in Neo4j with vector embeddings for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1f37e-e708-405b-90bb-c4a531af3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "# === Config ===\n",
    "PDF_DIR = \"/content/drive/MyDrive/Diabetes_KG_Project/2020_1\"\n",
    "\n",
    "# Embeddings BioBERT \n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Splitter (600 tokens + overlap 120 pour pr√©cision)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n",
    "\n",
    "# Lister les PDFs du dossier\n",
    "pdf_files = sorted([os.path.join(PDF_DIR, f) for f in os.listdir(PDF_DIR) if f.endswith(\".pdf\")])\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(f\"üìÇ Chargement de {len(pdf_files)} PDFs depuis {PDF_DIR}\")\n",
    "\n",
    "# 1. Charger et splitter chaque PDF\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pages = loader.load()\n",
    "    docs = splitter.split_documents(pages)\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "print(f\"üìù Nombre total de chunks : {len(all_docs)}\")\n",
    "\n",
    "# 2. Ins√©rer dans Neo4j\n",
    "vector_index = Neo4jVector.from_documents(\n",
    "    all_docs,\n",
    "    embedding_model,\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    node_label=\"Document\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Insertion termin√©e : {len(all_docs)} chunks ins√©r√©s dans Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc7105-33ff-4f6a-a5c9-2d99b9e11780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "# === Config ===\n",
    "PDF_DIR = \"/content/drive/MyDrive/Diabetes_KG_Project/2024_2\"\n",
    "\n",
    "# Embeddings BioBERT \n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Splitter (600 tokens + overlap 120  pour pr√©cision)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n",
    "\n",
    "# Lister les PDFs du dossier\n",
    "pdf_files = sorted([os.path.join(PDF_DIR, f) for f in os.listdir(PDF_DIR) if f.endswith(\".pdf\")])\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(f\"üìÇ Chargement de {len(pdf_files)} PDFs depuis {PDF_DIR}\")\n",
    "\n",
    "# 1. Charger et splitter chaque PDF\n",
    "for pdf in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf)\n",
    "        pages = loader.load()\n",
    "        docs = splitter.split_documents(pages)\n",
    "        if len(docs) > 1:   # ‚úÖ Supprimer uniquement le dernier chunk si >1\n",
    "            docs = docs[:-1]\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur avec {pdf}: {e}\")\n",
    "\n",
    "print(f\"üìù Nombre total de chunks (apr√®s suppression des derniers) : {len(all_docs)}\")\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(0, len(all_docs), batch_size):\n",
    "    batch = all_docs[i:i+batch_size]\n",
    "    vector_index = Neo4jVector.from_documents(\n",
    "        batch,\n",
    "        embedding_model,\n",
    "        url=url,\n",
    "        username=username,\n",
    "        password=password,\n",
    "        node_label=\"Document\",\n",
    "        text_node_property=\"text\",\n",
    "        embedding_node_property=\"embedding\"\n",
    "    )\n",
    "    print(f\"‚úÖ Batch {i//batch_size+1} ins√©r√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921114b4-bc01-45cd-884b-45089b5d568a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
